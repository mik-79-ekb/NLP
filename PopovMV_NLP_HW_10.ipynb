{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "4ghu98f8fbzydhaf4aq828",
    "id": "iRMUv0kofQ0z"
   },
   "source": [
    "## Введение в обработку естественного языка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "qak5vkvp2tsnvt5l7ohmz",
    "id": "n1sqh_xqfRzv"
   },
   "source": [
    "Домашнее задание №10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "vfqq25go71ebpo4so9zpnf",
    "id": "aSkQ3YdMfR2h"
   },
   "source": [
    "Урок 10. Машинный перевод. Модель seq2seq и механизм внимания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "8paxawnpv6udc3j7lftxnl",
    "id": "D5ma7MBAfR5Y"
   },
   "source": [
    "*Формат именования файла домашней работы: FIO_NLP_HW_N.ipynb, где N - номер домашнего задания*\n",
    "\n",
    "**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "duidacqt6zw1j2tjvf1pnfh",
    "id": "RrAgNsZ5fidi"
   },
   "source": [
    "Разобраться с моделью перевода (без механизма внимания) как она устроена, запустить для перевода с русского на английский (при желании можно взять другие пары языков)\n",
    "\n",
    "В данном практическом задании напишу и обучу модель перевода с русского на английский\n",
    "на библиотеке pytorch. Добавлю в неё механизм внимания, так как это сложнее и интереснее,\n",
    "а также рекуррентную сеть энкодера сделаю двунаправленной."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "7np42hm5rwmxb7d9vmeivd",
    "id": "J0Qjg6vuaHNt"
   },
   "source": [
    "Библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellId": "j3vzz9ph9fi7fiei4mzf8y",
    "id": "tnxXKDjq3jEL"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "u20gw0iqr6b08b6bmvmpgoh",
    "execution_id": "43a1787c-f180-49e5-90ff-0ef60cc00f67",
    "id": "wfodePkj3jEa"
   },
   "source": [
    "Датасет  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellId": "u1q1l50nxriodzb8rkkx",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CNvjhDyAKk3U",
    "outputId": "9ab8c05d-0789-432c-a80e-e64a8f2bc6ae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2023-05-17 15:25:41--  http://www.manythings.org/anki/rus-eng.zip\n",
      "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
      "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 15460248 (15M) [application/zip]\n",
      "Saving to: ‘rus-eng.zip’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  0%  136K 1m50s\n",
      "    50K .......... .......... .......... .......... ..........  0%  272K 83s\n",
      "   100K .......... .......... .......... .......... ..........  0%  516M 55s\n",
      "   150K .......... .......... .......... .......... ..........  1%  921M 41s\n",
      "   200K .......... .......... .......... .......... ..........  1%  272K 44s\n",
      "   250K .......... .......... .......... .......... ..........  1%  201M 36s\n",
      "   300K .......... .......... .......... .......... ..........  2%  132M 31s\n",
      "   350K .......... .......... .......... .......... ..........  2%  273K 34s\n",
      "   400K .......... .......... .......... .......... ..........  2%  149M 30s\n",
      "   450K .......... .......... .......... .......... ..........  3%  129M 27s\n",
      "   500K .......... .......... .......... .......... ..........  3%  818M 24s\n",
      "   550K .......... .......... .......... .......... ..........  3%  221M 22s\n",
      "   600K .......... .......... .......... .......... ..........  4%  685M 20s\n",
      "   650K .......... .......... .......... .......... ..........  4%  274K 23s\n",
      "   700K .......... .......... .......... .......... ..........  4%  264M 21s\n",
      "   750K .......... .......... .......... .......... ..........  5% 90.1M 20s\n",
      "   800K .......... .......... .......... .......... ..........  5%  746M 18s\n",
      "   850K .......... .......... .......... .......... ..........  5%  104M 17s\n",
      "   900K .......... .......... .......... .......... ..........  6%  415M 16s\n",
      "   950K .......... .......... .......... .......... ..........  6%  753M 16s\n",
      "  1000K .......... .......... .......... .......... ..........  6%  341M 15s\n",
      "  1050K .......... .......... .......... .......... ..........  7% 46.7M 14s\n",
      "  1100K .......... .......... .......... .......... ..........  7%  659M 13s\n",
      "  1150K .......... .......... .......... .......... ..........  7%  448M 13s\n",
      "  1200K .......... .......... .......... .......... ..........  8%  685M 12s\n",
      "  1250K .......... .......... .......... .......... ..........  8%  276K 14s\n",
      "  1300K .......... .......... .......... .......... ..........  8% 54.8M 13s\n",
      "  1350K .......... .......... .......... .......... ..........  9%  702M 13s\n",
      "  1400K .......... .......... .......... .......... ..........  9%  713M 12s\n",
      "  1450K .......... .......... .......... .......... ..........  9%  202M 12s\n",
      "  1500K .......... .......... .......... .......... .......... 10%  781M 11s\n",
      "  1550K .......... .......... .......... .......... .......... 10% 67.8M 11s\n",
      "  1600K .......... .......... .......... .......... .......... 10%  753M 10s\n",
      "  1650K .......... .......... .......... .......... .......... 11%  856M 10s\n",
      "  1700K .......... .......... .......... .......... .......... 11%  113M 10s\n",
      "  1750K .......... .......... .......... .......... .......... 11%  101M 10s\n",
      "  1800K .......... .......... .......... .......... .......... 12%  394M 9s\n",
      "  1850K .......... .......... .......... .......... .......... 12%  644M 9s\n",
      "  1900K .......... .......... .......... .......... .......... 12%  728M 9s\n",
      "  1950K .......... .......... .......... .......... .......... 13%  815M 8s\n",
      "  2000K .......... .......... .......... .......... .......... 13%  825M 8s\n",
      "  2050K .......... .......... .......... .......... .......... 13% 61.1M 8s\n",
      "  2100K .......... .......... .......... .......... .......... 14%  725M 8s\n",
      "  2150K .......... .......... .......... .......... .......... 14%  757M 8s\n",
      "  2200K .......... .......... .......... .......... .......... 14%  591M 7s\n",
      "  2250K .......... .......... .......... .......... .......... 15%  723M 7s\n",
      "  2300K .......... .......... .......... .......... .......... 15%  693M 7s\n",
      "  2350K .......... .......... .......... .......... .......... 15%  798M 7s\n",
      "  2400K .......... .......... .......... .......... .......... 16%  690M 7s\n",
      "  2450K .......... .......... .......... .......... .......... 16%  278K 7s\n",
      "  2500K .......... .......... .......... .......... .......... 16%  829M 7s\n",
      "  2550K .......... .......... .......... .......... .......... 17% 46.2M 7s\n",
      "  2600K .......... .......... .......... .......... .......... 17%  747M 7s\n",
      "  2650K .......... .......... .......... .......... .......... 17%  102M 7s\n",
      "  2700K .......... .......... .......... .......... .......... 18%  681M 7s\n",
      "  2750K .......... .......... .......... .......... .......... 18%  769M 6s\n",
      "  2800K .......... .......... .......... .......... .......... 18% 50.9M 6s\n",
      "  2850K .......... .......... .......... .......... .......... 19%  823M 6s\n",
      "  2900K .......... .......... .......... .......... .......... 19%  377M 6s\n",
      "  2950K .......... .......... .......... .......... .......... 19%  769M 6s\n",
      "  3000K .......... .......... .......... .......... .......... 20%  660M 6s\n",
      "  3050K .......... .......... .......... .......... .......... 20%  698M 6s\n",
      "  3100K .......... .......... .......... .......... .......... 20%  653M 6s\n",
      "  3150K .......... .......... .......... .......... .......... 21%  780M 5s\n",
      "  3200K .......... .......... .......... .......... .......... 21%  786M 5s\n",
      "  3250K .......... .......... .......... .......... .......... 21%  572M 5s\n",
      "  3300K .......... .......... .......... .......... .......... 22% 41.5M 5s\n",
      "  3350K .......... .......... .......... .......... .......... 22%  234M 5s\n",
      "  3400K .......... .......... .......... .......... .......... 22%  301M 5s\n",
      "  3450K .......... .......... .......... .......... .......... 23%  674M 5s\n",
      "  3500K .......... .......... .......... .......... .......... 23%  351M 5s\n",
      "  3550K .......... .......... .......... .......... .......... 23%  366M 5s\n",
      "  3600K .......... .......... .......... .......... .......... 24%  449M 5s\n",
      "  3650K .......... .......... .......... .......... .......... 24%  601M 5s\n",
      "  3700K .......... .......... .......... .......... .......... 24%  596M 4s\n",
      "  3750K .......... .......... .......... .......... .......... 25%  515M 4s\n",
      "  3800K .......... .......... .......... .......... .......... 25%  369M 4s\n",
      "  3850K .......... .......... .......... .......... .......... 25%  433M 4s\n",
      "  3900K .......... .......... .......... .......... .......... 26% 40.2M 4s\n",
      "  3950K .......... .......... .......... .......... .......... 26%  331M 4s\n",
      "  4000K .......... .......... .......... .......... .......... 26%  355M 4s\n",
      "  4050K .......... .......... .......... .......... .......... 27%  297M 4s\n",
      "  4100K .......... .......... .......... .......... .......... 27%  381M 4s\n",
      "  4150K .......... .......... .......... .......... .......... 27%  425M 4s\n",
      "  4200K .......... .......... .......... .......... .......... 28%  417M 4s\n",
      "  4250K .......... .......... .......... .......... .......... 28%  499M 4s\n",
      "  4300K .......... .......... .......... .......... .......... 28%  531M 4s\n",
      "  4350K .......... .......... .......... .......... .......... 29%  525M 4s\n",
      "  4400K .......... .......... .......... .......... .......... 29%  541M 4s\n",
      "  4450K .......... .......... .......... .......... .......... 29%  611M 3s\n",
      "  4500K .......... .......... .......... .......... .......... 30%  599M 3s\n",
      "  4550K .......... .......... .......... .......... .......... 30%  545M 3s\n",
      "  4600K .......... .......... .......... .......... .......... 30%  594M 3s\n",
      "  4650K .......... .......... .......... .......... .......... 31%  595M 3s\n",
      "  4700K .......... .......... .......... .......... .......... 31%  649M 3s\n",
      "  4750K .......... .......... .......... .......... .......... 31%  521M 3s\n",
      "  4800K .......... .......... .......... .......... .......... 32%  630M 3s\n",
      "  4850K .......... .......... .......... .......... .......... 32%  286K 3s\n",
      "  4900K .......... .......... .......... .......... .......... 32%  396M 3s\n",
      "  4950K .......... .......... .......... .......... .......... 33%  602M 3s\n",
      "  5000K .......... .......... .......... .......... .......... 33%  547M 3s\n",
      "  5050K .......... .......... .......... .......... .......... 33% 22.7M 3s\n",
      "  5100K .......... .......... .......... .......... .......... 34%  322M 3s\n",
      "  5150K .......... .......... .......... .......... .......... 34%  503M 3s\n",
      "  5200K .......... .......... .......... .......... .......... 34% 72.3M 3s\n",
      "  5250K .......... .......... .......... .......... .......... 35%  446M 3s\n",
      "  5300K .......... .......... .......... .......... .......... 35%  312M 3s\n",
      "  5350K .......... .......... .......... .......... .......... 35%  559M 3s\n",
      "  5400K .......... .......... .......... .......... .......... 36%  554M 3s\n",
      "  5450K .......... .......... .......... .......... .......... 36%  605M 3s\n",
      "  5500K .......... .......... .......... .......... .......... 36% 87.7M 3s\n",
      "  5550K .......... .......... .......... .......... .......... 37%  511M 3s\n",
      "  5600K .......... .......... .......... .......... .......... 37%  512M 3s\n",
      "  5650K .......... .......... .......... .......... .......... 37% 59.2M 3s\n",
      "  5700K .......... .......... .......... .......... .......... 38%  478M 3s\n",
      "  5750K .......... .......... .......... .......... .......... 38%  343M 3s\n",
      "  5800K .......... .......... .......... .......... .......... 38%  590M 3s\n",
      "  5850K .......... .......... .......... .......... .......... 39%  639M 3s\n",
      "  5900K .......... .......... .......... .......... .......... 39%  590M 3s\n",
      "  5950K .......... .......... .......... .......... .......... 39%  559M 3s\n",
      "  6000K .......... .......... .......... .......... .......... 40% 93.2M 2s\n",
      "  6050K .......... .......... .......... .......... .......... 40%  490M 2s\n",
      "  6100K .......... .......... .......... .......... .......... 40%  510M 2s\n",
      "  6150K .......... .......... .......... .......... .......... 41%  526M 2s\n",
      "  6200K .......... .......... .......... .......... .......... 41%  592M 2s\n",
      "  6250K .......... .......... .......... .......... .......... 41% 77.0M 2s\n",
      "  6300K .......... .......... .......... .......... .......... 42%  444M 2s\n",
      "  6350K .......... .......... .......... .......... .......... 42%  604M 2s\n",
      "  6400K .......... .......... .......... .......... .......... 42% 62.8M 2s\n",
      "  6450K .......... .......... .......... .......... .......... 43%  512M 2s\n",
      "  6500K .......... .......... .......... .......... .......... 43%  190M 2s\n",
      "  6550K .......... .......... .......... .......... .......... 43%  586M 2s\n",
      "  6600K .......... .......... .......... .......... .......... 44%  385M 2s\n",
      "  6650K .......... .......... .......... .......... .......... 44%  595M 2s\n",
      "  6700K .......... .......... .......... .......... .......... 44%  587M 2s\n",
      "  6750K .......... .......... .......... .......... .......... 45% 58.4M 2s\n",
      "  6800K .......... .......... .......... .......... .......... 45%  669M 2s\n",
      "  6850K .......... .......... .......... .......... .......... 45%  398M 2s\n",
      "  6900K .......... .......... .......... .......... .......... 46%  372M 2s\n",
      "  6950K .......... .......... .......... .......... .......... 46%  393M 2s\n",
      "  7000K .......... .......... .......... .......... .......... 46%  535M 2s\n",
      "  7050K .......... .......... .......... .......... .......... 47%  652M 2s\n",
      "  7100K .......... .......... .......... .......... .......... 47%  584M 2s\n",
      "  7150K .......... .......... .......... .......... .......... 47%  539M 2s\n",
      "  7200K .......... .......... .......... .......... .......... 48%  640M 2s\n",
      "  7250K .......... .......... .......... .......... .......... 48%  679M 2s\n",
      "  7300K .......... .......... .......... .......... .......... 48%  626M 2s\n",
      "  7350K .......... .......... .......... .......... .......... 49%  190M 2s\n",
      "  7400K .......... .......... .......... .......... .......... 49%  403M 2s\n",
      "  7450K .......... .......... .......... .......... .......... 49%  456M 2s\n",
      "  7500K .......... .......... .......... .......... .......... 50%  644M 2s\n",
      "  7550K .......... .......... .......... .......... .......... 50%  291K 2s\n",
      "  7600K .......... .......... .......... .......... .......... 50%  335M 2s\n",
      "  7650K .......... .......... .......... .......... .......... 51%  178M 2s\n",
      "  7700K .......... .......... .......... .......... .......... 51%  537M 2s\n",
      "  7750K .......... .......... .......... .......... .......... 51% 56.6M 2s\n",
      "  7800K .......... .......... .......... .......... .......... 51%  593M 2s\n",
      "  7850K .......... .......... .......... .......... .......... 52%  261M 2s\n",
      "  7900K .......... .......... .......... .......... .......... 52%  386M 2s\n",
      "  7950K .......... .......... .......... .......... .......... 52%  570M 2s\n",
      "  8000K .......... .......... .......... .......... .......... 53%  567M 2s\n",
      "  8050K .......... .......... .......... .......... .......... 53% 35.1M 2s\n",
      "  8100K .......... .......... .......... .......... .......... 53%  190M 2s\n",
      "  8150K .......... .......... .......... .......... .......... 54%  543M 2s\n",
      "  8200K .......... .......... .......... .......... .......... 54%  610M 2s\n",
      "  8250K .......... .......... .......... .......... .......... 54%  108M 2s\n",
      "  8300K .......... .......... .......... .......... .......... 55%  173M 1s\n",
      "  8350K .......... .......... .......... .......... .......... 55%  558M 1s\n",
      "  8400K .......... .......... .......... .......... .......... 55% 26.6M 1s\n",
      "  8450K .......... .......... .......... .......... .......... 56%  262M 1s\n",
      "  8500K .......... .......... .......... .......... .......... 56% 90.5M 1s\n",
      "  8550K .......... .......... .......... .......... .......... 56%  522M 1s\n",
      "  8600K .......... .......... .......... .......... .......... 57%  346M 1s\n",
      "  8650K .......... .......... .......... .......... .......... 57%  485M 1s\n",
      "  8700K .......... .......... .......... .......... .......... 57%  643M 1s\n",
      "  8750K .......... .......... .......... .......... .......... 58%  514M 1s\n",
      "  8800K .......... .......... .......... .......... .......... 58%  594M 1s\n",
      "  8850K .......... .......... .......... .......... .......... 58%  102M 1s\n",
      "  8900K .......... .......... .......... .......... .......... 59%  601M 1s\n",
      "  8950K .......... .......... .......... .......... .......... 59%  615M 1s\n",
      "  9000K .......... .......... .......... .......... .......... 59% 56.4M 1s\n",
      "  9050K .......... .......... .......... .......... .......... 60%  488M 1s\n",
      "  9100K .......... .......... .......... .......... .......... 60%  483M 1s\n",
      "  9150K .......... .......... .......... .......... .......... 60%  552M 1s\n",
      "  9200K .......... .......... .......... .......... .......... 61%  453M 1s\n",
      "  9250K .......... .......... .......... .......... .......... 61%  601M 1s\n",
      "  9300K .......... .......... .......... .......... .......... 61%  640M 1s\n",
      "  9350K .......... .......... .......... .......... .......... 62%  552M 1s\n",
      "  9400K .......... .......... .......... .......... .......... 62%  642M 1s\n",
      "  9450K .......... .......... .......... .......... .......... 62% 46.2M 1s\n",
      "  9500K .......... .......... .......... .......... .......... 63%  469M 1s\n",
      "  9550K .......... .......... .......... .......... .......... 63%  537M 1s\n",
      "  9600K .......... .......... .......... .......... .......... 63%  637M 1s\n",
      "  9650K .......... .......... .......... .......... .......... 64% 88.8M 1s\n",
      "  9700K .......... .......... .......... .......... .......... 64%  225M 1s\n",
      "  9750K .......... .......... .......... .......... .......... 64%  481M 1s\n",
      "  9800K .......... .......... .......... .......... .......... 65%  525M 1s\n",
      "  9850K .......... .......... .......... .......... .......... 65%  496M 1s\n",
      "  9900K .......... .......... .......... .......... .......... 65%  589M 1s\n",
      "  9950K .......... .......... .......... .......... .......... 66%  625M 1s\n",
      " 10000K .......... .......... .......... .......... .......... 66% 79.1M 1s\n",
      " 10050K .......... .......... .......... .......... .......... 66%  339M 1s\n",
      " 10100K .......... .......... .......... .......... .......... 67%  485M 1s\n",
      " 10150K .......... .......... .......... .......... .......... 67%  422M 1s\n",
      " 10200K .......... .......... .......... .......... .......... 67%  524M 1s\n",
      " 10250K .......... .......... .......... .......... .......... 68%  571M 1s\n",
      " 10300K .......... .......... .......... .......... .......... 68%  585M 1s\n",
      " 10350K .......... .......... .......... .......... .......... 68%  602M 1s\n",
      " 10400K .......... .......... .......... .......... .......... 69%  292K 1s\n",
      " 10450K .......... .......... .......... .......... .......... 69%  515M 1s\n",
      " 10500K .......... .......... .......... .......... .......... 69%  312M 1s\n",
      " 10550K .......... .......... .......... .......... .......... 70%  172M 1s\n",
      " 10600K .......... .......... .......... .......... .......... 70%  643M 1s\n",
      " 10650K .......... .......... .......... .......... .......... 70% 59.6M 1s\n",
      " 10700K .......... .......... .......... .......... .......... 71%  610M 1s\n",
      " 10750K .......... .......... .......... .......... .......... 71%  351M 1s\n",
      " 10800K .......... .......... .......... .......... .......... 71%  418M 1s\n",
      " 10850K .......... .......... .......... .......... .......... 72%  638M 1s\n",
      " 10900K .......... .......... .......... .......... .......... 72%  581M 1s\n",
      " 10950K .......... .......... .......... .......... .......... 72%  637M 1s\n",
      " 11000K .......... .......... .......... .......... .......... 73%  548M 1s\n",
      " 11050K .......... .......... .......... .......... .......... 73%  187M 1s\n",
      " 11100K .......... .......... .......... .......... .......... 73%  625M 1s\n",
      " 11150K .......... .......... .......... .......... .......... 74%  127M 1s\n",
      " 11200K .......... .......... .......... .......... .......... 74%  251M 1s\n",
      " 11250K .......... .......... .......... .......... .......... 74%  570M 1s\n",
      " 11300K .......... .......... .......... .......... .......... 75%  643M 1s\n",
      " 11350K .......... .......... .......... .......... .......... 75%  164M 1s\n",
      " 11400K .......... .......... .......... .......... .......... 75% 27.7M 1s\n",
      " 11450K .......... .......... .......... .......... .......... 76%  131M 1s\n",
      " 11500K .......... .......... .......... .......... .......... 76%  252M 1s\n",
      " 11550K .......... .......... .......... .......... .......... 76%  119M 1s\n",
      " 11600K .......... .......... .......... .......... .......... 77% 45.3M 1s\n",
      " 11650K .......... .......... .......... .......... .......... 77%  153M 1s\n",
      " 11700K .......... .......... .......... .......... .......... 77%  459M 1s\n",
      " 11750K .......... .......... .......... .......... .......... 78%  136M 1s\n",
      " 11800K .......... .......... .......... .......... .......... 78%  542M 1s\n",
      " 11850K .......... .......... .......... .......... .......... 78%  618M 1s\n",
      " 11900K .......... .......... .......... .......... .......... 79%  148M 1s\n",
      " 11950K .......... .......... .......... .......... .......... 79%  209M 1s\n",
      " 12000K .......... .......... .......... .......... .......... 79%  181M 1s\n",
      " 12050K .......... .......... .......... .......... .......... 80%  133M 1s\n",
      " 12100K .......... .......... .......... .......... .......... 80%  213M 0s\n",
      " 12150K .......... .......... .......... .......... .......... 80%  373M 0s\n",
      " 12200K .......... .......... .......... .......... .......... 81%  446M 0s\n",
      " 12250K .......... .......... .......... .......... .......... 81%  402M 0s\n",
      " 12300K .......... .......... .......... .......... .......... 81%  176M 0s\n",
      " 12350K .......... .......... .......... .......... .......... 82%  491M 0s\n",
      " 12400K .......... .......... .......... .......... .......... 82%  115M 0s\n",
      " 12450K .......... .......... .......... .......... .......... 82%  389M 0s\n",
      " 12500K .......... .......... .......... .......... .......... 83%  495M 0s\n",
      " 12550K .......... .......... .......... .......... .......... 83%  642M 0s\n",
      " 12600K .......... .......... .......... .......... .......... 83% 83.1M 0s\n",
      " 12650K .......... .......... .......... .......... .......... 84%  617M 0s\n",
      " 12700K .......... .......... .......... .......... .......... 84%  136M 0s\n",
      " 12750K .......... .......... .......... .......... .......... 84%  783M 0s\n",
      " 12800K .......... .......... .......... .......... .......... 85%  118M 0s\n",
      " 12850K .......... .......... .......... .......... .......... 85%  323M 0s\n",
      " 12900K .......... .......... .......... .......... .......... 85%  806M 0s\n",
      " 12950K .......... .......... .......... .......... .......... 86%  444M 0s\n",
      " 13000K .......... .......... .......... .......... .......... 86% 51.0M 0s\n",
      " 13050K .......... .......... .......... .......... .......... 86%  262M 0s\n",
      " 13100K .......... .......... .......... .......... .......... 87%  352M 0s\n",
      " 13150K .......... .......... .......... .......... .......... 87%  923M 0s\n",
      " 13200K .......... .......... .......... .......... .......... 87%  409M 0s\n",
      " 13250K .......... .......... .......... .......... .......... 88%  979M 0s\n",
      " 13300K .......... .......... .......... .......... .......... 88%  293K 0s\n",
      " 13350K .......... .......... .......... .......... .......... 88%  591M 0s\n",
      " 13400K .......... .......... .......... .......... .......... 89% 77.9M 0s\n",
      " 13450K .......... .......... .......... .......... .......... 89%  788M 0s\n",
      " 13500K .......... .......... .......... .......... .......... 89%  276M 0s\n",
      " 13550K .......... .......... .......... .......... .......... 90%  399M 0s\n",
      " 13600K .......... .......... .......... .......... .......... 90%  886M 0s\n",
      " 13650K .......... .......... .......... .......... .......... 90%  446M 0s\n",
      " 13700K .......... .......... .......... .......... .......... 91%  133M 0s\n",
      " 13750K .......... .......... .......... .......... .......... 91%  832M 0s\n",
      " 13800K .......... .......... .......... .......... .......... 91%  336M 0s\n",
      " 13850K .......... .......... .......... .......... .......... 92%  782M 0s\n",
      " 13900K .......... .......... .......... .......... .......... 92% 57.0M 0s\n",
      " 13950K .......... .......... .......... .......... .......... 92%  347M 0s\n",
      " 14000K .......... .......... .......... .......... .......... 93%  790M 0s\n",
      " 14050K .......... .......... .......... .......... .......... 93%  378M 0s\n",
      " 14100K .......... .......... .......... .......... .......... 93%  778M 0s\n",
      " 14150K .......... .......... .......... .......... .......... 94%  320M 0s\n",
      " 14200K .......... .......... .......... .......... .......... 94%  823M 0s\n",
      " 14250K .......... .......... .......... .......... .......... 94%  485M 0s\n",
      " 14300K .......... .......... .......... .......... .......... 95%  592M 0s\n",
      " 14350K .......... .......... .......... .......... .......... 95%  463M 0s\n",
      " 14400K .......... .......... .......... .......... .......... 95%  526M 0s\n",
      " 14450K .......... .......... .......... .......... .......... 96%  139M 0s\n",
      " 14500K .......... .......... .......... .......... .......... 96% 77.6M 0s\n",
      " 14550K .......... .......... .......... .......... .......... 96%  448M 0s\n",
      " 14600K .......... .......... .......... .......... .......... 97% 71.1M 0s\n",
      " 14650K .......... .......... .......... .......... .......... 97% 49.1M 0s\n",
      " 14700K .......... .......... .......... .......... .......... 97% 95.8M 0s\n",
      " 14750K .......... .......... .......... .......... .......... 98%  295M 0s\n",
      " 14800K .......... .......... .......... .......... .......... 98%  106M 0s\n",
      " 14850K .......... .......... .......... .......... .......... 98%  691M 0s\n",
      " 14900K .......... .......... .......... .......... .......... 99%  388M 0s\n",
      " 14950K .......... .......... .......... .......... .......... 99%  709M 0s\n",
      " 15000K .......... .......... .......... .......... .......... 99% 83.4M 0s\n",
      " 15050K .......... .......... .......... .......... .......   100%  758M=2.2s\n",
      "\n",
      "2023-05-17 15:25:44 (6.65 MB/s) - ‘rus-eng.zip’ saved [15460248/15460248]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.manythings.org/anki/rus-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellId": "i8vixqbiw88iby768mwuyr",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "83bg17Lr-7XK",
    "outputId": "e8aff900-28f5-42c5-86fd-a2b56f818a28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  rus-eng.zip\n",
      "  inflating: rus-eng/rus.txt         \n",
      "  inflating: rus-eng/_about.txt      \n"
     ]
    }
   ],
   "source": [
    "!mkdir rus-eng\n",
    "!unzip rus-eng.zip -d rus-eng/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellId": "jdg1l15azetp59rr5qzyh",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7o5L92efMMhf",
    "outputId": "8aec2ed7-5cc0-4b0b-f2a2-388a5a809108"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 74M\n",
      "drwxr-xr-x 1 jupyter jupyter   34 May 17 15:26 .\n",
      "drwxr-xr-x 1 jupyter jupyter  118 May 17 15:26 ..\n",
      "-rw-r--r-- 1 jupyter jupyter 1.5K Apr  2 03:16 _about.txt\n",
      "-rw-r--r-- 1 jupyter jupyter  74M Apr  2 03:16 rus.txt\n"
     ]
    }
   ],
   "source": [
    "!ls ./rus-eng/ -lah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellId": "3jyui9htf7t24bvq7hej25",
    "id": "kRVATYOgJs1b"
   },
   "outputs": [],
   "source": [
    "# Download the file\n",
    "path_to_file = \"./rus-eng/rus.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "cellId": "f2clgs9tf4mwv0by5ncvyn",
    "id": "rd0jw-eC3jEh"
   },
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "  w = w.lower().strip()\n",
    "\n",
    "  # creating a space between a word and the punctuation following it\n",
    "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "  w = re.sub(r\"([?.!,])\", r\" \\1 \", w)\n",
    "  w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "  w = re.sub(r\"[^a-zA-Zа-яА-Я?.!,']+\", \" \", w)\n",
    "\n",
    "  w = w.strip()\n",
    "\n",
    "  # adding a start and an end token to the sentence\n",
    "  # so that the model know when to start and stop predicting.\n",
    "  w = '<start> ' + w + ' <end>'\n",
    "  return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "cellId": "nebfguud10mbffu1c5rg37",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "yV9lZXQXNbnH",
    "outputId": "33a95b3d-583e-41e9-ca4a-a75bc3d72955"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> i can't fly like a bird . <end>\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_sentence(\"I can't fly like a bird.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "cellId": "8ag2t429jdkak5kpif6mf5",
    "id": "OHn4Dct23jEm"
   },
   "outputs": [],
   "source": [
    "# 1. Remove the accents\n",
    "# 2. Clean the sentences\n",
    "# 3. Return word pairs in the format: [ENG, RUS]\n",
    "def create_dataset(path, num_examples):\n",
    "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')[:2]]  for l in lines[:num_examples]]\n",
    "\n",
    "  return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "cellId": "n6msruu8l36ryps8toq0j",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cTbSbBz55QtF",
    "outputId": "14a7c10d-7bb4-411d-ec78-afd0794ee45a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> wait ! <end>\n",
      "<start> стой ! <end>\n"
     ]
    }
   ],
   "source": [
    "en, ru = create_dataset(path_to_file, None)\n",
    "print(en[42])\n",
    "print(ru[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "cellId": "zx7eedklq6qx0xx51ftws",
    "id": "bIOn8RCNDJXG"
   },
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "\n",
    "  return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "cellId": "g8lcsooapqbn0wgvhsrqd",
    "id": "eAY9k49G3jE_"
   },
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "  # creating cleaned input, output pairs\n",
    "  targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "\n",
    "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "mrn1za5yw7k7xd1d80t2fh",
    "execution_id": "74d0b024-c0bc-47d4-80cb-bbaf6a7ad7d9",
    "id": "GOi42V79Ydlr"
   },
   "source": [
    "### Limit the size of the dataset to experiment faster (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "cellId": "hlnz858as1s190cezmtpqs",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C8j9g9AnIeZV",
    "outputId": "8fd11bc0-0980-4499-9027-3c587046830b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(467119, 467119)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en), len(ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "cellId": "83tek5eb3mn97584qyo9su",
    "id": "cnxC7q-j3jFD"
   },
   "outputs": [],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "num_examples = 300000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "cellId": "82fhzcewdlispv9rxhn0wi",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4QILQkOs3jFG",
    "outputId": "01bfb815-d2cc-4939-f427-5a70577ba3d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240000 240000 60000 60000\n"
     ]
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "cellId": "qwpiuia2idn497m1e9pqhd",
    "id": "lJPmLZGMeD5q"
   },
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "  for t in tensor:\n",
    "    if t!=0:\n",
    "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "cellId": "qoc3z1hnvbnnc88ipmv08",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VXukARTDd7MT",
    "outputId": "d9eb642a-2ad9-48c9-d992-5b100c98d10b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "11 ----> ты\n",
      "2850 ----> скучаешь\n",
      "43 ----> по\n",
      "558 ----> своим\n",
      "1536 ----> детям\n",
      "6 ----> ?\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "12 ----> do\n",
      "6 ----> you\n",
      "322 ----> miss\n",
      "25 ----> your\n",
      "446 ----> kids\n",
      "7 ----> ?\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "wfwmjv2z7hgkiwm95uu1tf",
    "execution_id": "ae87222c-8b25-4d56-bd74-a214ca43deee",
    "id": "rgCLkfv5uO3d"
   },
   "source": [
    "### Create a tf.data dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "cellId": "p6il17xtdiqs91nfe75sjg",
    "id": "TqHsArVZ3jFS"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 200\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "cellId": "qhztmyba21im02y6isju",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qc6-NK1GtWQt",
    "outputId": "19ad5d88-f3a5-4ae7-888c-3328a953c758"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 18]), TensorShape([64, 13]))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "cellId": "5srvdud9elwrytidprz66i",
    "id": "nZ2rI24i3jFg"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.enc_units = enc_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=False,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    \n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state = hidden)\n",
    "    return state\n",
    "\n",
    "  def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "cellId": "xy1kipq5wgj7ksujbo3p",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "60gSVh05Jl6l",
    "outputId": "b8b22c97-b8e0-40b9-c066-1356ff7c4ce0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 15:42:13.940640: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "# print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "cellId": "wultrfkptahu3ge7o3ky5",
    "id": "yJ_B3mhW3jFk"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x, initial_state=hidden)\n",
    "\n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size, vocab)\n",
    "    x = self.fc(output)\n",
    "\n",
    "    return x, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "cellId": "0a4oxa34vd9gv0f487frkh",
    "id": "P5UY8wko3jFp"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "decoder_sample_x, decoder_sample_h = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "cellId": "fcfwnrpscwwt1aj17j0p4d",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XKcypC0AGeLR",
    "outputId": "a089b9fc-f448-454c-e94d-4eb11015e999"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 12598])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "decoder_sample_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "cellId": "axes1ni73lqsagtdpisdyd",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6y0HF-zMF_vp",
    "outputId": "23fcd91b-9cc4-4ea6-93fd-b321d84c11fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 1024])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "decoder_sample_h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "xti35dd53h43qbkkfranr",
    "execution_id": "d48f4793-8568-4b33-bc1f-761b0c4c8949",
    "id": "_ch_71VbIRfK"
   },
   "source": [
    "## Define the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "cellId": "u62re92y7elllspn41amt",
    "id": "WmTHr5iV3jFr"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0)) # маска будет зануляиь нулевые токены паддинга , \n",
    "            #через которые мы не хотим прокидывать градиент, \n",
    "            #поэтому мы их сразу в лоссе зануляем чтобы они на лосс никак не реагировали, \n",
    "            #чтобы не вносили джобавочные коэффициенты в наш лосс\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "ul2hs55i7enodv6zkdiph",
    "execution_id": "55d1142b-8faa-4a12-8ecc-c59b2d69ca55",
    "id": "DMVWzzsfNl4e"
   },
   "source": [
    "## Checkpoints (Object-based saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "cellId": "vm6vn4myzdhc2o7e41uogj",
    "id": "Zj8bXQTgNwrF"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "checkpoint_dir = './training_nmt_checkpoints'\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "cellId": "e9zk2n25828fjpb9g9sa6",
    "id": "sC9ArXSsVfqn"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    for t in range(1, targ.shape[1]):\n",
    "      # passing enc_output to the decoder\n",
    "      predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
    "\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "      # using teacher forcing\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "cellId": "xmih6hkf0kyeq53txahos",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 571
    },
    "id": "ddefjBMa3jF0",
    "outputId": "07252a9f-2841-46d6-83c0-1d54e62aa023"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 15:42:54.444284: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 5.0723\n",
      "Epoch 1 Batch 100 Loss 2.4849\n",
      "Epoch 1 Batch 200 Loss 2.1388\n",
      "Epoch 1 Batch 300 Loss 2.0370\n",
      "Epoch 1 Batch 400 Loss 1.8355\n",
      "Epoch 1 Batch 500 Loss 1.8472\n",
      "Epoch 1 Batch 600 Loss 1.7284\n",
      "Epoch 1 Batch 700 Loss 1.4948\n",
      "Epoch 1 Batch 800 Loss 1.4560\n",
      "Epoch 1 Batch 900 Loss 1.4138\n",
      "Epoch 1 Batch 1000 Loss 1.3458\n",
      "Epoch 1 Batch 1100 Loss 1.2454\n",
      "Epoch 1 Batch 1200 Loss 1.2433\n",
      "Epoch 1 Batch 1300 Loss 1.1861\n",
      "Epoch 1 Batch 1400 Loss 1.3452\n",
      "Epoch 1 Batch 1500 Loss 1.1816\n",
      "Epoch 1 Batch 1600 Loss 1.0039\n",
      "Epoch 1 Batch 1700 Loss 1.0431\n",
      "Epoch 1 Batch 1800 Loss 0.9384\n",
      "Epoch 1 Batch 1900 Loss 1.0764\n",
      "Epoch 1 Batch 2000 Loss 1.0756\n",
      "Epoch 1 Batch 2100 Loss 0.9501\n",
      "Epoch 1 Batch 2200 Loss 1.0049\n",
      "Epoch 1 Batch 2300 Loss 0.9930\n",
      "Epoch 1 Batch 2400 Loss 0.7928\n",
      "Epoch 1 Batch 2500 Loss 0.9512\n",
      "Epoch 1 Batch 2600 Loss 0.7285\n",
      "Epoch 1 Batch 2700 Loss 0.7570\n",
      "Epoch 1 Batch 2800 Loss 0.8158\n",
      "Epoch 1 Batch 2900 Loss 0.8160\n",
      "Epoch 1 Batch 3000 Loss 0.7842\n",
      "Epoch 1 Batch 3100 Loss 0.7642\n",
      "Epoch 1 Batch 3200 Loss 0.6394\n",
      "Epoch 1 Batch 3300 Loss 0.6743\n",
      "Epoch 1 Batch 3400 Loss 0.6278\n",
      "Epoch 1 Batch 3500 Loss 0.6288\n",
      "Epoch 1 Batch 3600 Loss 0.5308\n",
      "Epoch 1 Batch 3700 Loss 0.7267\n",
      "Epoch 1 Loss 1.1585\n",
      "Time taken for 1 epoch 122.48529195785522 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.4664\n",
      "Epoch 2 Batch 100 Loss 0.5728\n",
      "Epoch 2 Batch 200 Loss 0.5080\n",
      "Epoch 2 Batch 300 Loss 0.4953\n",
      "Epoch 2 Batch 400 Loss 0.5010\n",
      "Epoch 2 Batch 500 Loss 0.4659\n",
      "Epoch 2 Batch 600 Loss 0.4457\n",
      "Epoch 2 Batch 700 Loss 0.4476\n",
      "Epoch 2 Batch 800 Loss 0.4552\n",
      "Epoch 2 Batch 900 Loss 0.4153\n",
      "Epoch 2 Batch 1000 Loss 0.4509\n",
      "Epoch 2 Batch 1100 Loss 0.4913\n",
      "Epoch 2 Batch 1200 Loss 0.4971\n",
      "Epoch 2 Batch 1300 Loss 0.4480\n",
      "Epoch 2 Batch 1400 Loss 0.4656\n",
      "Epoch 2 Batch 1500 Loss 0.3774\n",
      "Epoch 2 Batch 1600 Loss 0.4392\n",
      "Epoch 2 Batch 1700 Loss 0.4950\n",
      "Epoch 2 Batch 1800 Loss 0.4783\n",
      "Epoch 2 Batch 1900 Loss 0.3497\n",
      "Epoch 2 Batch 2000 Loss 0.3959\n",
      "Epoch 2 Batch 2100 Loss 0.2801\n",
      "Epoch 2 Batch 2200 Loss 0.5153\n",
      "Epoch 2 Batch 2300 Loss 0.3996\n",
      "Epoch 2 Batch 2400 Loss 0.3800\n",
      "Epoch 2 Batch 2500 Loss 0.3542\n",
      "Epoch 2 Batch 2600 Loss 0.4973\n",
      "Epoch 2 Batch 2700 Loss 0.3930\n",
      "Epoch 2 Batch 2800 Loss 0.4201\n",
      "Epoch 2 Batch 2900 Loss 0.4087\n",
      "Epoch 2 Batch 3000 Loss 0.4332\n",
      "Epoch 2 Batch 3100 Loss 0.5049\n",
      "Epoch 2 Batch 3200 Loss 0.4702\n",
      "Epoch 2 Batch 3300 Loss 0.4028\n",
      "Epoch 2 Batch 3400 Loss 0.4709\n",
      "Epoch 2 Batch 3500 Loss 0.3140\n",
      "Epoch 2 Batch 3600 Loss 0.4117\n",
      "Epoch 2 Batch 3700 Loss 0.3724\n",
      "Epoch 2 Loss 0.4426\n",
      "Time taken for 1 epoch 110.37841057777405 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.2631\n",
      "Epoch 3 Batch 100 Loss 0.2290\n",
      "Epoch 3 Batch 200 Loss 0.2593\n",
      "Epoch 3 Batch 300 Loss 0.2335\n",
      "Epoch 3 Batch 400 Loss 0.3046\n",
      "Epoch 3 Batch 500 Loss 0.3303\n",
      "Epoch 3 Batch 600 Loss 0.3200\n",
      "Epoch 3 Batch 700 Loss 0.2249\n",
      "Epoch 3 Batch 800 Loss 0.2920\n",
      "Epoch 3 Batch 900 Loss 0.2803\n",
      "Epoch 3 Batch 1000 Loss 0.2260\n",
      "Epoch 3 Batch 1100 Loss 0.3283\n",
      "Epoch 3 Batch 1200 Loss 0.2744\n",
      "Epoch 3 Batch 1300 Loss 0.2517\n",
      "Epoch 3 Batch 1400 Loss 0.3072\n",
      "Epoch 3 Batch 1500 Loss 0.2370\n",
      "Epoch 3 Batch 1600 Loss 0.2715\n",
      "Epoch 3 Batch 1700 Loss 0.3038\n",
      "Epoch 3 Batch 1800 Loss 0.2771\n",
      "Epoch 3 Batch 1900 Loss 0.2141\n",
      "Epoch 3 Batch 2000 Loss 0.2528\n",
      "Epoch 3 Batch 2100 Loss 0.2805\n",
      "Epoch 3 Batch 2200 Loss 0.3085\n",
      "Epoch 3 Batch 2300 Loss 0.2695\n",
      "Epoch 3 Batch 2400 Loss 0.2582\n",
      "Epoch 3 Batch 2500 Loss 0.3635\n",
      "Epoch 3 Batch 2600 Loss 0.3166\n",
      "Epoch 3 Batch 2700 Loss 0.2878\n",
      "Epoch 3 Batch 2800 Loss 0.2231\n",
      "Epoch 3 Batch 2900 Loss 0.2307\n",
      "Epoch 3 Batch 3000 Loss 0.3156\n",
      "Epoch 3 Batch 3100 Loss 0.3209\n",
      "Epoch 3 Batch 3200 Loss 0.3033\n",
      "Epoch 3 Batch 3300 Loss 0.2907\n",
      "Epoch 3 Batch 3400 Loss 0.3624\n",
      "Epoch 3 Batch 3500 Loss 0.2034\n",
      "Epoch 3 Batch 3600 Loss 0.3355\n",
      "Epoch 3 Batch 3700 Loss 0.2623\n",
      "Epoch 3 Loss 0.2695\n",
      "Time taken for 1 epoch 107.93088221549988 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.2023\n",
      "Epoch 4 Batch 100 Loss 0.1500\n",
      "Epoch 4 Batch 200 Loss 0.1477\n",
      "Epoch 4 Batch 300 Loss 0.1894\n",
      "Epoch 4 Batch 400 Loss 0.1399\n",
      "Epoch 4 Batch 500 Loss 0.1681\n",
      "Epoch 4 Batch 600 Loss 0.1651\n",
      "Epoch 4 Batch 700 Loss 0.1893\n",
      "Epoch 4 Batch 800 Loss 0.1494\n",
      "Epoch 4 Batch 900 Loss 0.2437\n",
      "Epoch 4 Batch 1000 Loss 0.1987\n",
      "Epoch 4 Batch 1100 Loss 0.1915\n",
      "Epoch 4 Batch 1200 Loss 0.1944\n",
      "Epoch 4 Batch 1300 Loss 0.2453\n",
      "Epoch 4 Batch 1400 Loss 0.1957\n",
      "Epoch 4 Batch 1500 Loss 0.1667\n",
      "Epoch 4 Batch 1600 Loss 0.1680\n",
      "Epoch 4 Batch 1700 Loss 0.1938\n",
      "Epoch 4 Batch 1800 Loss 0.1909\n",
      "Epoch 4 Batch 1900 Loss 0.1911\n",
      "Epoch 4 Batch 2000 Loss 0.1911\n",
      "Epoch 4 Batch 2100 Loss 0.2428\n",
      "Epoch 4 Batch 2200 Loss 0.2603\n",
      "Epoch 4 Batch 2300 Loss 0.2022\n",
      "Epoch 4 Batch 2400 Loss 0.1509\n",
      "Epoch 4 Batch 2500 Loss 0.2521\n",
      "Epoch 4 Batch 2600 Loss 0.2104\n",
      "Epoch 4 Batch 2700 Loss 0.2069\n",
      "Epoch 4 Batch 2800 Loss 0.1994\n",
      "Epoch 4 Batch 2900 Loss 0.2092\n",
      "Epoch 4 Batch 3000 Loss 0.2446\n",
      "Epoch 4 Batch 3100 Loss 0.2020\n",
      "Epoch 4 Batch 3200 Loss 0.2728\n",
      "Epoch 4 Batch 3300 Loss 0.2445\n",
      "Epoch 4 Batch 3400 Loss 0.2216\n",
      "Epoch 4 Batch 3500 Loss 0.1543\n",
      "Epoch 4 Batch 3600 Loss 0.2574\n",
      "Epoch 4 Batch 3700 Loss 0.1937\n",
      "Epoch 4 Loss 0.1940\n",
      "Time taken for 1 epoch 109.91241455078125 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.1416\n",
      "Epoch 5 Batch 100 Loss 0.1081\n",
      "Epoch 5 Batch 200 Loss 0.1307\n",
      "Epoch 5 Batch 300 Loss 0.1338\n",
      "Epoch 5 Batch 400 Loss 0.1346\n",
      "Epoch 5 Batch 500 Loss 0.1474\n",
      "Epoch 5 Batch 600 Loss 0.1207\n",
      "Epoch 5 Batch 700 Loss 0.1534\n",
      "Epoch 5 Batch 800 Loss 0.1526\n",
      "Epoch 5 Batch 900 Loss 0.1555\n",
      "Epoch 5 Batch 1000 Loss 0.1506\n",
      "Epoch 5 Batch 1100 Loss 0.1248\n",
      "Epoch 5 Batch 1200 Loss 0.2344\n",
      "Epoch 5 Batch 1300 Loss 0.1706\n",
      "Epoch 5 Batch 1400 Loss 0.1953\n",
      "Epoch 5 Batch 1500 Loss 0.1627\n",
      "Epoch 5 Batch 1600 Loss 0.1401\n",
      "Epoch 5 Batch 1700 Loss 0.1260\n",
      "Epoch 5 Batch 1800 Loss 0.1742\n",
      "Epoch 5 Batch 1900 Loss 0.1925\n",
      "Epoch 5 Batch 2000 Loss 0.1927\n",
      "Epoch 5 Batch 2100 Loss 0.1598\n",
      "Epoch 5 Batch 2200 Loss 0.1463\n",
      "Epoch 5 Batch 2300 Loss 0.1620\n",
      "Epoch 5 Batch 2400 Loss 0.1495\n",
      "Epoch 5 Batch 2500 Loss 0.1536\n",
      "Epoch 5 Batch 2600 Loss 0.1597\n",
      "Epoch 5 Batch 2700 Loss 0.2149\n",
      "Epoch 5 Batch 2800 Loss 0.2040\n",
      "Epoch 5 Batch 2900 Loss 0.1884\n",
      "Epoch 5 Batch 3000 Loss 0.1875\n",
      "Epoch 5 Batch 3100 Loss 0.1953\n",
      "Epoch 5 Batch 3200 Loss 0.1860\n",
      "Epoch 5 Batch 3300 Loss 0.1739\n",
      "Epoch 5 Batch 3400 Loss 0.1415\n",
      "Epoch 5 Batch 3500 Loss 0.1283\n",
      "Epoch 5 Batch 3600 Loss 0.2573\n",
      "Epoch 5 Batch 3700 Loss 0.1656\n",
      "Epoch 5 Loss 0.1573\n",
      "Time taken for 1 epoch 107.9190285205841 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.0933\n",
      "Epoch 6 Batch 100 Loss 0.1281\n",
      "Epoch 6 Batch 200 Loss 0.1311\n",
      "Epoch 6 Batch 300 Loss 0.0954\n",
      "Epoch 6 Batch 400 Loss 0.1380\n",
      "Epoch 6 Batch 500 Loss 0.1201\n",
      "Epoch 6 Batch 600 Loss 0.0990\n",
      "Epoch 6 Batch 700 Loss 0.1155\n",
      "Epoch 6 Batch 800 Loss 0.1375\n",
      "Epoch 6 Batch 900 Loss 0.0989\n",
      "Epoch 6 Batch 1000 Loss 0.1210\n",
      "Epoch 6 Batch 1100 Loss 0.1280\n",
      "Epoch 6 Batch 1200 Loss 0.1246\n",
      "Epoch 6 Batch 1300 Loss 0.1613\n",
      "Epoch 6 Batch 1400 Loss 0.1156\n",
      "Epoch 6 Batch 1500 Loss 0.1282\n",
      "Epoch 6 Batch 1600 Loss 0.1087\n",
      "Epoch 6 Batch 1700 Loss 0.1477\n",
      "Epoch 6 Batch 1800 Loss 0.1274\n",
      "Epoch 6 Batch 1900 Loss 0.1599\n",
      "Epoch 6 Batch 2000 Loss 0.1161\n",
      "Epoch 6 Batch 2100 Loss 0.0987\n",
      "Epoch 6 Batch 2200 Loss 0.1642\n",
      "Epoch 6 Batch 2300 Loss 0.1337\n",
      "Epoch 6 Batch 2400 Loss 0.1893\n",
      "Epoch 6 Batch 2500 Loss 0.1842\n",
      "Epoch 6 Batch 2600 Loss 0.1153\n",
      "Epoch 6 Batch 2700 Loss 0.1339\n",
      "Epoch 6 Batch 2800 Loss 0.0905\n",
      "Epoch 6 Batch 2900 Loss 0.1610\n",
      "Epoch 6 Batch 3000 Loss 0.1171\n",
      "Epoch 6 Batch 3100 Loss 0.1587\n",
      "Epoch 6 Batch 3200 Loss 0.1491\n",
      "Epoch 6 Batch 3300 Loss 0.1228\n",
      "Epoch 6 Batch 3400 Loss 0.1622\n",
      "Epoch 6 Batch 3500 Loss 0.1467\n",
      "Epoch 6 Batch 3600 Loss 0.1648\n",
      "Epoch 6 Batch 3700 Loss 0.1312\n",
      "Epoch 6 Loss 0.1373\n",
      "Time taken for 1 epoch 110.36650681495667 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.1116\n",
      "Epoch 7 Batch 100 Loss 0.1010\n",
      "Epoch 7 Batch 200 Loss 0.1027\n",
      "Epoch 7 Batch 300 Loss 0.0988\n",
      "Epoch 7 Batch 400 Loss 0.0930\n",
      "Epoch 7 Batch 500 Loss 0.0834\n",
      "Epoch 7 Batch 600 Loss 0.1149\n",
      "Epoch 7 Batch 700 Loss 0.1003\n",
      "Epoch 7 Batch 800 Loss 0.1101\n",
      "Epoch 7 Batch 900 Loss 0.1152\n",
      "Epoch 7 Batch 1000 Loss 0.1564\n",
      "Epoch 7 Batch 1100 Loss 0.1039\n",
      "Epoch 7 Batch 1200 Loss 0.1550\n",
      "Epoch 7 Batch 1300 Loss 0.0905\n",
      "Epoch 7 Batch 1400 Loss 0.1222\n",
      "Epoch 7 Batch 1500 Loss 0.1795\n",
      "Epoch 7 Batch 1600 Loss 0.1024\n",
      "Epoch 7 Batch 1700 Loss 0.1314\n",
      "Epoch 7 Batch 1800 Loss 0.1029\n",
      "Epoch 7 Batch 1900 Loss 0.1512\n",
      "Epoch 7 Batch 2000 Loss 0.1563\n",
      "Epoch 7 Batch 2100 Loss 0.1435\n",
      "Epoch 7 Batch 2200 Loss 0.1426\n",
      "Epoch 7 Batch 2300 Loss 0.0968\n",
      "Epoch 7 Batch 2400 Loss 0.1495\n",
      "Epoch 7 Batch 2500 Loss 0.1171\n",
      "Epoch 7 Batch 2600 Loss 0.1315\n",
      "Epoch 7 Batch 2700 Loss 0.1358\n",
      "Epoch 7 Batch 2800 Loss 0.1371\n",
      "Epoch 7 Batch 2900 Loss 0.1621\n",
      "Epoch 7 Batch 3000 Loss 0.1345\n",
      "Epoch 7 Batch 3100 Loss 0.1595\n",
      "Epoch 7 Batch 3200 Loss 0.1330\n",
      "Epoch 7 Batch 3300 Loss 0.1355\n",
      "Epoch 7 Batch 3400 Loss 0.1387\n",
      "Epoch 7 Batch 3500 Loss 0.1575\n",
      "Epoch 7 Batch 3600 Loss 0.1313\n",
      "Epoch 7 Batch 3700 Loss 0.1703\n",
      "Epoch 7 Loss 0.1258\n",
      "Time taken for 1 epoch 107.66006064414978 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.0812\n",
      "Epoch 8 Batch 100 Loss 0.1227\n",
      "Epoch 8 Batch 200 Loss 0.1061\n",
      "Epoch 8 Batch 300 Loss 0.1051\n",
      "Epoch 8 Batch 400 Loss 0.0821\n",
      "Epoch 8 Batch 500 Loss 0.0658\n",
      "Epoch 8 Batch 600 Loss 0.1194\n",
      "Epoch 8 Batch 700 Loss 0.0900\n",
      "Epoch 8 Batch 800 Loss 0.1222\n",
      "Epoch 8 Batch 900 Loss 0.0934\n",
      "Epoch 8 Batch 1000 Loss 0.1304\n",
      "Epoch 8 Batch 1100 Loss 0.0979\n",
      "Epoch 8 Batch 1200 Loss 0.1339\n",
      "Epoch 8 Batch 1300 Loss 0.0958\n",
      "Epoch 8 Batch 1400 Loss 0.0984\n",
      "Epoch 8 Batch 1500 Loss 0.1080\n",
      "Epoch 8 Batch 1600 Loss 0.1171\n",
      "Epoch 8 Batch 1700 Loss 0.1217\n",
      "Epoch 8 Batch 1800 Loss 0.1347\n",
      "Epoch 8 Batch 1900 Loss 0.1082\n",
      "Epoch 8 Batch 2000 Loss 0.0763\n",
      "Epoch 8 Batch 2100 Loss 0.1363\n",
      "Epoch 8 Batch 2200 Loss 0.1329\n",
      "Epoch 8 Batch 2300 Loss 0.1182\n",
      "Epoch 8 Batch 2400 Loss 0.1043\n",
      "Epoch 8 Batch 2500 Loss 0.1502\n",
      "Epoch 8 Batch 2600 Loss 0.1151\n",
      "Epoch 8 Batch 2700 Loss 0.1033\n",
      "Epoch 8 Batch 2800 Loss 0.1171\n",
      "Epoch 8 Batch 2900 Loss 0.1092\n",
      "Epoch 8 Batch 3000 Loss 0.1191\n",
      "Epoch 8 Batch 3100 Loss 0.1346\n",
      "Epoch 8 Batch 3200 Loss 0.1161\n",
      "Epoch 8 Batch 3300 Loss 0.1415\n",
      "Epoch 8 Batch 3400 Loss 0.1674\n",
      "Epoch 8 Batch 3500 Loss 0.1315\n",
      "Epoch 8 Batch 3600 Loss 0.1325\n",
      "Epoch 8 Batch 3700 Loss 0.1408\n",
      "Epoch 8 Loss 0.1177\n",
      "Time taken for 1 epoch 110.14137554168701 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.1221\n",
      "Epoch 9 Batch 100 Loss 0.1112\n",
      "Epoch 9 Batch 200 Loss 0.0779\n",
      "Epoch 9 Batch 300 Loss 0.1052\n",
      "Epoch 9 Batch 400 Loss 0.0608\n",
      "Epoch 9 Batch 500 Loss 0.0712\n",
      "Epoch 9 Batch 600 Loss 0.0765\n",
      "Epoch 9 Batch 700 Loss 0.1155\n",
      "Epoch 9 Batch 800 Loss 0.0917\n",
      "Epoch 9 Batch 900 Loss 0.1325\n",
      "Epoch 9 Batch 1000 Loss 0.0796\n",
      "Epoch 9 Batch 1100 Loss 0.0731\n",
      "Epoch 9 Batch 1200 Loss 0.0971\n",
      "Epoch 9 Batch 1300 Loss 0.0805\n",
      "Epoch 9 Batch 1400 Loss 0.1046\n",
      "Epoch 9 Batch 1500 Loss 0.0922\n",
      "Epoch 9 Batch 1600 Loss 0.0844\n",
      "Epoch 9 Batch 1700 Loss 0.0945\n",
      "Epoch 9 Batch 1800 Loss 0.0950\n",
      "Epoch 9 Batch 1900 Loss 0.1038\n",
      "Epoch 9 Batch 2000 Loss 0.0900\n",
      "Epoch 9 Batch 2100 Loss 0.1103\n",
      "Epoch 9 Batch 2200 Loss 0.1472\n",
      "Epoch 9 Batch 2300 Loss 0.1548\n",
      "Epoch 9 Batch 2400 Loss 0.1257\n",
      "Epoch 9 Batch 2500 Loss 0.1170\n",
      "Epoch 9 Batch 2600 Loss 0.1125\n",
      "Epoch 9 Batch 2700 Loss 0.1407\n",
      "Epoch 9 Batch 2800 Loss 0.1232\n",
      "Epoch 9 Batch 2900 Loss 0.1290\n",
      "Epoch 9 Batch 3000 Loss 0.0886\n",
      "Epoch 9 Batch 3100 Loss 0.0950\n",
      "Epoch 9 Batch 3200 Loss 0.1386\n",
      "Epoch 9 Batch 3300 Loss 0.0822\n",
      "Epoch 9 Batch 3400 Loss 0.1304\n",
      "Epoch 9 Batch 3500 Loss 0.0910\n",
      "Epoch 9 Batch 3600 Loss 0.1033\n",
      "Epoch 9 Batch 3700 Loss 0.1350\n",
      "Epoch 9 Loss 0.1120\n",
      "Time taken for 1 epoch 107.59649419784546 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.1112\n",
      "Epoch 10 Batch 100 Loss 0.1134\n",
      "Epoch 10 Batch 200 Loss 0.0824\n",
      "Epoch 10 Batch 300 Loss 0.0893\n",
      "Epoch 10 Batch 400 Loss 0.1129\n",
      "Epoch 10 Batch 500 Loss 0.0634\n",
      "Epoch 10 Batch 600 Loss 0.1167\n",
      "Epoch 10 Batch 700 Loss 0.0723\n",
      "Epoch 10 Batch 800 Loss 0.0992\n",
      "Epoch 10 Batch 900 Loss 0.0842\n",
      "Epoch 10 Batch 1000 Loss 0.1425\n",
      "Epoch 10 Batch 1100 Loss 0.1026\n",
      "Epoch 10 Batch 1200 Loss 0.0804\n",
      "Epoch 10 Batch 1300 Loss 0.0689\n",
      "Epoch 10 Batch 1400 Loss 0.1276\n",
      "Epoch 10 Batch 1500 Loss 0.0984\n",
      "Epoch 10 Batch 1600 Loss 0.0948\n",
      "Epoch 10 Batch 1700 Loss 0.1416\n",
      "Epoch 10 Batch 1800 Loss 0.0924\n",
      "Epoch 10 Batch 1900 Loss 0.0784\n",
      "Epoch 10 Batch 2000 Loss 0.1508\n",
      "Epoch 10 Batch 2100 Loss 0.0902\n",
      "Epoch 10 Batch 2200 Loss 0.1090\n",
      "Epoch 10 Batch 2300 Loss 0.1361\n",
      "Epoch 10 Batch 2400 Loss 0.1223\n",
      "Epoch 10 Batch 2500 Loss 0.0833\n",
      "Epoch 10 Batch 2600 Loss 0.1102\n",
      "Epoch 10 Batch 2700 Loss 0.1204\n",
      "Epoch 10 Batch 2800 Loss 0.0944\n",
      "Epoch 10 Batch 2900 Loss 0.1058\n",
      "Epoch 10 Batch 3000 Loss 0.1423\n",
      "Epoch 10 Batch 3100 Loss 0.1367\n",
      "Epoch 10 Batch 3200 Loss 0.1345\n",
      "Epoch 10 Batch 3300 Loss 0.1104\n",
      "Epoch 10 Batch 3400 Loss 0.1660\n",
      "Epoch 10 Batch 3500 Loss 0.1267\n",
      "Epoch 10 Batch 3600 Loss 0.1475\n",
      "Epoch 10 Batch 3700 Loss 0.0873\n",
      "Epoch 10 Loss 0.1085\n",
      "Time taken for 1 epoch 109.67545771598816 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.0700\n",
      "Epoch 11 Batch 100 Loss 0.0693\n",
      "Epoch 11 Batch 200 Loss 0.1349\n",
      "Epoch 11 Batch 300 Loss 0.0893\n",
      "Epoch 11 Batch 400 Loss 0.0648\n",
      "Epoch 11 Batch 500 Loss 0.1175\n",
      "Epoch 11 Batch 600 Loss 0.0889\n",
      "Epoch 11 Batch 700 Loss 0.0806\n",
      "Epoch 11 Batch 800 Loss 0.0923\n",
      "Epoch 11 Batch 900 Loss 0.0782\n",
      "Epoch 11 Batch 1000 Loss 0.0755\n",
      "Epoch 11 Batch 1100 Loss 0.0895\n",
      "Epoch 11 Batch 1200 Loss 0.0988\n",
      "Epoch 11 Batch 1300 Loss 0.1020\n",
      "Epoch 11 Batch 1400 Loss 0.1364\n",
      "Epoch 11 Batch 1500 Loss 0.0961\n",
      "Epoch 11 Batch 1600 Loss 0.1130\n",
      "Epoch 11 Batch 1700 Loss 0.0689\n",
      "Epoch 11 Batch 1800 Loss 0.1218\n",
      "Epoch 11 Batch 1900 Loss 0.1232\n",
      "Epoch 11 Batch 2000 Loss 0.0888\n",
      "Epoch 11 Batch 2100 Loss 0.1107\n",
      "Epoch 11 Batch 2200 Loss 0.1289\n",
      "Epoch 11 Batch 2300 Loss 0.1435\n",
      "Epoch 11 Batch 2400 Loss 0.1010\n",
      "Epoch 11 Batch 2500 Loss 0.1148\n",
      "Epoch 11 Batch 2600 Loss 0.0802\n",
      "Epoch 11 Batch 2700 Loss 0.1055\n",
      "Epoch 11 Batch 2800 Loss 0.1132\n",
      "Epoch 11 Batch 2900 Loss 0.1346\n",
      "Epoch 11 Batch 3000 Loss 0.1175\n",
      "Epoch 11 Batch 3100 Loss 0.1208\n",
      "Epoch 11 Batch 3200 Loss 0.1113\n",
      "Epoch 11 Batch 3300 Loss 0.1312\n",
      "Epoch 11 Batch 3400 Loss 0.1338\n",
      "Epoch 11 Batch 3500 Loss 0.1016\n",
      "Epoch 11 Batch 3600 Loss 0.0948\n",
      "Epoch 11 Batch 3700 Loss 0.1221\n",
      "Epoch 11 Loss 0.1059\n",
      "Time taken for 1 epoch 107.61598920822144 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.0923\n",
      "Epoch 12 Batch 100 Loss 0.0695\n",
      "Epoch 12 Batch 200 Loss 0.0443\n",
      "Epoch 12 Batch 300 Loss 0.0864\n",
      "Epoch 12 Batch 400 Loss 0.0915\n",
      "Epoch 12 Batch 500 Loss 0.0619\n",
      "Epoch 12 Batch 600 Loss 0.0921\n",
      "Epoch 12 Batch 700 Loss 0.1143\n",
      "Epoch 12 Batch 800 Loss 0.0796\n",
      "Epoch 12 Batch 900 Loss 0.0683\n",
      "Epoch 12 Batch 1000 Loss 0.1200\n",
      "Epoch 12 Batch 1100 Loss 0.0854\n",
      "Epoch 12 Batch 1200 Loss 0.1135\n",
      "Epoch 12 Batch 1300 Loss 0.0956\n",
      "Epoch 12 Batch 1400 Loss 0.1070\n",
      "Epoch 12 Batch 1500 Loss 0.1252\n",
      "Epoch 12 Batch 1600 Loss 0.1177\n",
      "Epoch 12 Batch 1700 Loss 0.1296\n",
      "Epoch 12 Batch 1800 Loss 0.0658\n",
      "Epoch 12 Batch 1900 Loss 0.1275\n",
      "Epoch 12 Batch 2000 Loss 0.1320\n",
      "Epoch 12 Batch 2100 Loss 0.0749\n",
      "Epoch 12 Batch 2200 Loss 0.1237\n",
      "Epoch 12 Batch 2300 Loss 0.1262\n",
      "Epoch 12 Batch 2400 Loss 0.0933\n",
      "Epoch 12 Batch 2500 Loss 0.1230\n",
      "Epoch 12 Batch 2600 Loss 0.0790\n",
      "Epoch 12 Batch 2700 Loss 0.0900\n",
      "Epoch 12 Batch 2800 Loss 0.0700\n",
      "Epoch 12 Batch 2900 Loss 0.1130\n",
      "Epoch 12 Batch 3000 Loss 0.1068\n",
      "Epoch 12 Batch 3100 Loss 0.0789\n",
      "Epoch 12 Batch 3200 Loss 0.0857\n",
      "Epoch 12 Batch 3300 Loss 0.0769\n",
      "Epoch 12 Batch 3400 Loss 0.1211\n",
      "Epoch 12 Batch 3500 Loss 0.1089\n",
      "Epoch 12 Batch 3600 Loss 0.0967\n",
      "Epoch 12 Batch 3700 Loss 0.1173\n",
      "Epoch 12 Loss 0.1034\n",
      "Time taken for 1 epoch 109.81894040107727 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.1094\n",
      "Epoch 13 Batch 100 Loss 0.0835\n",
      "Epoch 13 Batch 200 Loss 0.0736\n",
      "Epoch 13 Batch 300 Loss 0.0658\n",
      "Epoch 13 Batch 400 Loss 0.0747\n",
      "Epoch 13 Batch 500 Loss 0.1337\n",
      "Epoch 13 Batch 600 Loss 0.0632\n",
      "Epoch 13 Batch 700 Loss 0.0658\n",
      "Epoch 13 Batch 800 Loss 0.0884\n",
      "Epoch 13 Batch 900 Loss 0.0684\n",
      "Epoch 13 Batch 1000 Loss 0.0789\n",
      "Epoch 13 Batch 1100 Loss 0.0873\n",
      "Epoch 13 Batch 1200 Loss 0.0815\n",
      "Epoch 13 Batch 1300 Loss 0.1294\n",
      "Epoch 13 Batch 1400 Loss 0.0823\n",
      "Epoch 13 Batch 1500 Loss 0.1083\n",
      "Epoch 13 Batch 1600 Loss 0.1180\n",
      "Epoch 13 Batch 1700 Loss 0.1056\n",
      "Epoch 13 Batch 1800 Loss 0.1513\n",
      "Epoch 13 Batch 1900 Loss 0.1247\n",
      "Epoch 13 Batch 2000 Loss 0.0924\n",
      "Epoch 13 Batch 2100 Loss 0.1211\n",
      "Epoch 13 Batch 2200 Loss 0.0882\n",
      "Epoch 13 Batch 2300 Loss 0.1246\n",
      "Epoch 13 Batch 2400 Loss 0.1273\n",
      "Epoch 13 Batch 2500 Loss 0.1151\n",
      "Epoch 13 Batch 2600 Loss 0.1146\n",
      "Epoch 13 Batch 2700 Loss 0.1357\n",
      "Epoch 13 Batch 2800 Loss 0.1142\n",
      "Epoch 13 Batch 2900 Loss 0.0954\n",
      "Epoch 13 Batch 3000 Loss 0.1243\n",
      "Epoch 13 Batch 3100 Loss 0.1430\n",
      "Epoch 13 Batch 3200 Loss 0.1535\n",
      "Epoch 13 Batch 3300 Loss 0.1204\n",
      "Epoch 13 Batch 3400 Loss 0.1158\n",
      "Epoch 13 Batch 3500 Loss 0.1423\n",
      "Epoch 13 Batch 3600 Loss 0.1058\n",
      "Epoch 13 Batch 3700 Loss 0.1229\n",
      "Epoch 13 Loss 0.1018\n",
      "Time taken for 1 epoch 107.74577617645264 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.0606\n",
      "Epoch 14 Batch 100 Loss 0.0647\n",
      "Epoch 14 Batch 200 Loss 0.0674\n",
      "Epoch 14 Batch 300 Loss 0.0679\n",
      "Epoch 14 Batch 400 Loss 0.0473\n",
      "Epoch 14 Batch 500 Loss 0.1008\n",
      "Epoch 14 Batch 600 Loss 0.0901\n",
      "Epoch 14 Batch 700 Loss 0.0862\n",
      "Epoch 14 Batch 800 Loss 0.0907\n",
      "Epoch 14 Batch 900 Loss 0.0822\n",
      "Epoch 14 Batch 1000 Loss 0.0755\n",
      "Epoch 14 Batch 1100 Loss 0.1013\n",
      "Epoch 14 Batch 1200 Loss 0.1006\n",
      "Epoch 14 Batch 1300 Loss 0.1022\n",
      "Epoch 14 Batch 1400 Loss 0.1214\n",
      "Epoch 14 Batch 1500 Loss 0.0716\n",
      "Epoch 14 Batch 1600 Loss 0.0950\n",
      "Epoch 14 Batch 1700 Loss 0.0733\n",
      "Epoch 14 Batch 1800 Loss 0.0959\n",
      "Epoch 14 Batch 1900 Loss 0.0864\n",
      "Epoch 14 Batch 2000 Loss 0.0727\n",
      "Epoch 14 Batch 2100 Loss 0.1126\n",
      "Epoch 14 Batch 2200 Loss 0.1226\n",
      "Epoch 14 Batch 2300 Loss 0.0925\n",
      "Epoch 14 Batch 2400 Loss 0.0962\n",
      "Epoch 14 Batch 2500 Loss 0.1257\n",
      "Epoch 14 Batch 2600 Loss 0.0782\n",
      "Epoch 14 Batch 2700 Loss 0.0986\n",
      "Epoch 14 Batch 2800 Loss 0.1407\n",
      "Epoch 14 Batch 2900 Loss 0.1427\n",
      "Epoch 14 Batch 3000 Loss 0.0862\n",
      "Epoch 14 Batch 3100 Loss 0.0961\n",
      "Epoch 14 Batch 3200 Loss 0.1755\n",
      "Epoch 14 Batch 3300 Loss 0.1405\n",
      "Epoch 14 Batch 3400 Loss 0.1437\n",
      "Epoch 14 Batch 3500 Loss 0.1128\n",
      "Epoch 14 Batch 3600 Loss 0.1016\n",
      "Epoch 14 Batch 3700 Loss 0.1506\n",
      "Epoch 14 Loss 0.1015\n",
      "Time taken for 1 epoch 109.59836149215698 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.0627\n",
      "Epoch 15 Batch 100 Loss 0.0669\n",
      "Epoch 15 Batch 200 Loss 0.0717\n",
      "Epoch 15 Batch 300 Loss 0.1095\n",
      "Epoch 15 Batch 400 Loss 0.0814\n",
      "Epoch 15 Batch 500 Loss 0.0675\n",
      "Epoch 15 Batch 600 Loss 0.0786\n",
      "Epoch 15 Batch 700 Loss 0.1153\n",
      "Epoch 15 Batch 800 Loss 0.0826\n",
      "Epoch 15 Batch 900 Loss 0.0842\n",
      "Epoch 15 Batch 1000 Loss 0.0773\n",
      "Epoch 15 Batch 1100 Loss 0.1009\n",
      "Epoch 15 Batch 1200 Loss 0.0898\n",
      "Epoch 15 Batch 1300 Loss 0.1160\n",
      "Epoch 15 Batch 1400 Loss 0.1316\n",
      "Epoch 15 Batch 1500 Loss 0.0756\n",
      "Epoch 15 Batch 1600 Loss 0.0921\n",
      "Epoch 15 Batch 1700 Loss 0.1293\n",
      "Epoch 15 Batch 1800 Loss 0.0784\n",
      "Epoch 15 Batch 1900 Loss 0.1241\n",
      "Epoch 15 Batch 2000 Loss 0.1338\n",
      "Epoch 15 Batch 2100 Loss 0.1193\n",
      "Epoch 15 Batch 2200 Loss 0.1035\n",
      "Epoch 15 Batch 2300 Loss 0.0932\n",
      "Epoch 15 Batch 2400 Loss 0.1262\n",
      "Epoch 15 Batch 2500 Loss 0.1062\n",
      "Epoch 15 Batch 2600 Loss 0.1075\n",
      "Epoch 15 Batch 2700 Loss 0.1067\n",
      "Epoch 15 Batch 2800 Loss 0.1553\n",
      "Epoch 15 Batch 2900 Loss 0.0946\n",
      "Epoch 15 Batch 3000 Loss 0.1133\n",
      "Epoch 15 Batch 3100 Loss 0.0994\n",
      "Epoch 15 Batch 3200 Loss 0.0914\n",
      "Epoch 15 Batch 3300 Loss 0.1433\n",
      "Epoch 15 Batch 3400 Loss 0.1316\n",
      "Epoch 15 Batch 3500 Loss 0.0963\n",
      "Epoch 15 Batch 3600 Loss 0.1369\n",
      "Epoch 15 Batch 3700 Loss 0.1237\n",
      "Epoch 15 Loss 0.1009\n",
      "Time taken for 1 epoch 107.60548067092896 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.0736\n",
      "Epoch 16 Batch 100 Loss 0.0613\n",
      "Epoch 16 Batch 200 Loss 0.0897\n",
      "Epoch 16 Batch 300 Loss 0.0690\n",
      "Epoch 16 Batch 400 Loss 0.0908\n",
      "Epoch 16 Batch 500 Loss 0.0577\n",
      "Epoch 16 Batch 600 Loss 0.0873\n",
      "Epoch 16 Batch 700 Loss 0.0639\n",
      "Epoch 16 Batch 800 Loss 0.0777\n",
      "Epoch 16 Batch 900 Loss 0.1155\n",
      "Epoch 16 Batch 1000 Loss 0.0962\n",
      "Epoch 16 Batch 1100 Loss 0.1078\n",
      "Epoch 16 Batch 1200 Loss 0.0489\n",
      "Epoch 16 Batch 1300 Loss 0.0551\n",
      "Epoch 16 Batch 1400 Loss 0.0905\n",
      "Epoch 16 Batch 1500 Loss 0.1311\n",
      "Epoch 16 Batch 1600 Loss 0.1204\n",
      "Epoch 16 Batch 1700 Loss 0.0806\n",
      "Epoch 16 Batch 1800 Loss 0.1038\n",
      "Epoch 16 Batch 1900 Loss 0.1094\n",
      "Epoch 16 Batch 2000 Loss 0.0848\n",
      "Epoch 16 Batch 2100 Loss 0.0991\n",
      "Epoch 16 Batch 2200 Loss 0.0843\n",
      "Epoch 16 Batch 2300 Loss 0.1408\n",
      "Epoch 16 Batch 2400 Loss 0.0959\n",
      "Epoch 16 Batch 2500 Loss 0.0728\n",
      "Epoch 16 Batch 2600 Loss 0.1280\n",
      "Epoch 16 Batch 2700 Loss 0.1418\n",
      "Epoch 16 Batch 2800 Loss 0.1032\n",
      "Epoch 16 Batch 2900 Loss 0.1181\n",
      "Epoch 16 Batch 3000 Loss 0.0820\n",
      "Epoch 16 Batch 3100 Loss 0.1205\n",
      "Epoch 16 Batch 3200 Loss 0.1107\n",
      "Epoch 16 Batch 3300 Loss 0.1223\n",
      "Epoch 16 Batch 3400 Loss 0.0904\n",
      "Epoch 16 Batch 3500 Loss 0.1253\n",
      "Epoch 16 Batch 3600 Loss 0.0885\n",
      "Epoch 16 Batch 3700 Loss 0.1257\n",
      "Epoch 16 Loss 0.0994\n",
      "Time taken for 1 epoch 109.86680150032043 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.0731\n",
      "Epoch 17 Batch 100 Loss 0.0803\n",
      "Epoch 17 Batch 200 Loss 0.0953\n",
      "Epoch 17 Batch 300 Loss 0.0842\n",
      "Epoch 17 Batch 400 Loss 0.0764\n",
      "Epoch 17 Batch 500 Loss 0.1160\n",
      "Epoch 17 Batch 600 Loss 0.0700\n",
      "Epoch 17 Batch 700 Loss 0.0780\n",
      "Epoch 17 Batch 800 Loss 0.1116\n",
      "Epoch 17 Batch 900 Loss 0.0933\n",
      "Epoch 17 Batch 1000 Loss 0.0734\n",
      "Epoch 17 Batch 1100 Loss 0.1198\n",
      "Epoch 17 Batch 1200 Loss 0.0750\n",
      "Epoch 17 Batch 1300 Loss 0.0990\n",
      "Epoch 17 Batch 1400 Loss 0.0700\n",
      "Epoch 17 Batch 1500 Loss 0.1423\n",
      "Epoch 17 Batch 1600 Loss 0.0975\n",
      "Epoch 17 Batch 1700 Loss 0.0755\n",
      "Epoch 17 Batch 1800 Loss 0.1010\n",
      "Epoch 17 Batch 1900 Loss 0.1349\n",
      "Epoch 17 Batch 2000 Loss 0.0771\n",
      "Epoch 17 Batch 2100 Loss 0.1192\n",
      "Epoch 17 Batch 2200 Loss 0.1285\n",
      "Epoch 17 Batch 2300 Loss 0.1097\n",
      "Epoch 17 Batch 2400 Loss 0.0863\n",
      "Epoch 17 Batch 2500 Loss 0.1238\n",
      "Epoch 17 Batch 2600 Loss 0.1118\n",
      "Epoch 17 Batch 2700 Loss 0.1339\n",
      "Epoch 17 Batch 2800 Loss 0.1234\n",
      "Epoch 17 Batch 2900 Loss 0.1265\n",
      "Epoch 17 Batch 3000 Loss 0.0898\n",
      "Epoch 17 Batch 3100 Loss 0.1002\n",
      "Epoch 17 Batch 3200 Loss 0.1313\n",
      "Epoch 17 Batch 3300 Loss 0.1024\n",
      "Epoch 17 Batch 3400 Loss 0.1225\n",
      "Epoch 17 Batch 3500 Loss 0.1054\n",
      "Epoch 17 Batch 3600 Loss 0.1095\n",
      "Epoch 17 Batch 3700 Loss 0.1515\n",
      "Epoch 17 Loss 0.0995\n",
      "Time taken for 1 epoch 107.53645396232605 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.0772\n",
      "Epoch 18 Batch 100 Loss 0.0796\n",
      "Epoch 18 Batch 200 Loss 0.0720\n",
      "Epoch 18 Batch 300 Loss 0.1040\n",
      "Epoch 18 Batch 400 Loss 0.0936\n",
      "Epoch 18 Batch 500 Loss 0.0834\n",
      "Epoch 18 Batch 600 Loss 0.0959\n",
      "Epoch 18 Batch 700 Loss 0.1147\n",
      "Epoch 18 Batch 800 Loss 0.1092\n",
      "Epoch 18 Batch 900 Loss 0.0647\n",
      "Epoch 18 Batch 1000 Loss 0.0700\n",
      "Epoch 18 Batch 1100 Loss 0.1096\n",
      "Epoch 18 Batch 1200 Loss 0.1008\n",
      "Epoch 18 Batch 1300 Loss 0.0744\n",
      "Epoch 18 Batch 1400 Loss 0.0845\n",
      "Epoch 18 Batch 1500 Loss 0.0955\n",
      "Epoch 18 Batch 1600 Loss 0.0683\n",
      "Epoch 18 Batch 1700 Loss 0.0648\n",
      "Epoch 18 Batch 1800 Loss 0.1202\n",
      "Epoch 18 Batch 1900 Loss 0.0870\n",
      "Epoch 18 Batch 2000 Loss 0.0888\n",
      "Epoch 18 Batch 2100 Loss 0.0998\n",
      "Epoch 18 Batch 2200 Loss 0.0746\n",
      "Epoch 18 Batch 2300 Loss 0.1178\n",
      "Epoch 18 Batch 2400 Loss 0.0825\n",
      "Epoch 18 Batch 2500 Loss 0.0863\n",
      "Epoch 18 Batch 2600 Loss 0.1043\n",
      "Epoch 18 Batch 2700 Loss 0.0990\n",
      "Epoch 18 Batch 2800 Loss 0.0997\n",
      "Epoch 18 Batch 2900 Loss 0.1152\n",
      "Epoch 18 Batch 3000 Loss 0.0955\n",
      "Epoch 18 Batch 3100 Loss 0.0514\n",
      "Epoch 18 Batch 3200 Loss 0.1023\n",
      "Epoch 18 Batch 3300 Loss 0.0871\n",
      "Epoch 18 Batch 3400 Loss 0.1424\n",
      "Epoch 18 Batch 3500 Loss 0.1497\n",
      "Epoch 18 Batch 3600 Loss 0.1191\n",
      "Epoch 18 Batch 3700 Loss 0.1106\n",
      "Epoch 18 Loss 0.0983\n",
      "Time taken for 1 epoch 109.6916868686676 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.1013\n",
      "Epoch 19 Batch 100 Loss 0.0531\n",
      "Epoch 19 Batch 200 Loss 0.0784\n",
      "Epoch 19 Batch 300 Loss 0.0547\n",
      "Epoch 19 Batch 400 Loss 0.0825\n",
      "Epoch 19 Batch 500 Loss 0.0938\n",
      "Epoch 19 Batch 600 Loss 0.0572\n",
      "Epoch 19 Batch 700 Loss 0.0686\n",
      "Epoch 19 Batch 800 Loss 0.0811\n",
      "Epoch 19 Batch 900 Loss 0.0821\n",
      "Epoch 19 Batch 1000 Loss 0.1237\n",
      "Epoch 19 Batch 1100 Loss 0.0687\n",
      "Epoch 19 Batch 1200 Loss 0.0980\n",
      "Epoch 19 Batch 1300 Loss 0.0910\n",
      "Epoch 19 Batch 1400 Loss 0.0635\n",
      "Epoch 19 Batch 1500 Loss 0.0602\n",
      "Epoch 19 Batch 1600 Loss 0.0777\n",
      "Epoch 19 Batch 1700 Loss 0.0802\n",
      "Epoch 19 Batch 1800 Loss 0.1065\n",
      "Epoch 19 Batch 1900 Loss 0.0920\n",
      "Epoch 19 Batch 2000 Loss 0.1019\n",
      "Epoch 19 Batch 2100 Loss 0.0953\n",
      "Epoch 19 Batch 2200 Loss 0.0893\n",
      "Epoch 19 Batch 2300 Loss 0.1178\n",
      "Epoch 19 Batch 2400 Loss 0.1056\n",
      "Epoch 19 Batch 2500 Loss 0.1420\n",
      "Epoch 19 Batch 2600 Loss 0.1263\n",
      "Epoch 19 Batch 2700 Loss 0.1527\n",
      "Epoch 19 Batch 2800 Loss 0.1029\n",
      "Epoch 19 Batch 2900 Loss 0.0783\n",
      "Epoch 19 Batch 3000 Loss 0.0995\n",
      "Epoch 19 Batch 3100 Loss 0.1341\n",
      "Epoch 19 Batch 3200 Loss 0.0738\n",
      "Epoch 19 Batch 3300 Loss 0.1226\n",
      "Epoch 19 Batch 3400 Loss 0.0938\n",
      "Epoch 19 Batch 3500 Loss 0.1182\n",
      "Epoch 19 Batch 3600 Loss 0.1292\n",
      "Epoch 19 Batch 3700 Loss 0.0789\n",
      "Epoch 19 Loss 0.0988\n",
      "Time taken for 1 epoch 107.47442579269409 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.1166\n",
      "Epoch 20 Batch 100 Loss 0.0838\n",
      "Epoch 20 Batch 200 Loss 0.0894\n",
      "Epoch 20 Batch 300 Loss 0.0698\n",
      "Epoch 20 Batch 400 Loss 0.0691\n",
      "Epoch 20 Batch 500 Loss 0.0812\n",
      "Epoch 20 Batch 600 Loss 0.0794\n",
      "Epoch 20 Batch 700 Loss 0.0930\n",
      "Epoch 20 Batch 800 Loss 0.1174\n",
      "Epoch 20 Batch 900 Loss 0.0813\n",
      "Epoch 20 Batch 1000 Loss 0.0858\n",
      "Epoch 20 Batch 1100 Loss 0.0639\n",
      "Epoch 20 Batch 1200 Loss 0.0871\n",
      "Epoch 20 Batch 1300 Loss 0.0849\n",
      "Epoch 20 Batch 1400 Loss 0.0720\n",
      "Epoch 20 Batch 1500 Loss 0.0999\n",
      "Epoch 20 Batch 1600 Loss 0.1055\n",
      "Epoch 20 Batch 1700 Loss 0.0850\n",
      "Epoch 20 Batch 1800 Loss 0.0736\n",
      "Epoch 20 Batch 1900 Loss 0.0786\n",
      "Epoch 20 Batch 2000 Loss 0.0775\n",
      "Epoch 20 Batch 2100 Loss 0.1024\n",
      "Epoch 20 Batch 2200 Loss 0.0947\n",
      "Epoch 20 Batch 2300 Loss 0.0678\n",
      "Epoch 20 Batch 2400 Loss 0.1278\n",
      "Epoch 20 Batch 2500 Loss 0.0977\n",
      "Epoch 20 Batch 2600 Loss 0.1032\n",
      "Epoch 20 Batch 2700 Loss 0.0991\n",
      "Epoch 20 Batch 2800 Loss 0.0942\n",
      "Epoch 20 Batch 2900 Loss 0.1275\n",
      "Epoch 20 Batch 3000 Loss 0.0950\n",
      "Epoch 20 Batch 3100 Loss 0.1166\n",
      "Epoch 20 Batch 3200 Loss 0.1130\n",
      "Epoch 20 Batch 3300 Loss 0.1258\n",
      "Epoch 20 Batch 3400 Loss 0.1127\n",
      "Epoch 20 Batch 3500 Loss 0.1501\n",
      "Epoch 20 Batch 3600 Loss 0.0714\n",
      "Epoch 20 Batch 3700 Loss 0.1099\n",
      "Epoch 20 Loss 0.0989\n",
      "Time taken for 1 epoch 109.72988390922546 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 0.0899\n",
      "Epoch 21 Batch 100 Loss 0.0756\n",
      "Epoch 21 Batch 200 Loss 0.0718\n",
      "Epoch 21 Batch 300 Loss 0.0740\n",
      "Epoch 21 Batch 400 Loss 0.1035\n",
      "Epoch 21 Batch 500 Loss 0.0810\n",
      "Epoch 21 Batch 600 Loss 0.0685\n",
      "Epoch 21 Batch 700 Loss 0.0918\n",
      "Epoch 21 Batch 800 Loss 0.0414\n",
      "Epoch 21 Batch 900 Loss 0.1249\n",
      "Epoch 21 Batch 1000 Loss 0.0976\n",
      "Epoch 21 Batch 1100 Loss 0.0824\n",
      "Epoch 21 Batch 1200 Loss 0.0835\n",
      "Epoch 21 Batch 1300 Loss 0.1012\n",
      "Epoch 21 Batch 1400 Loss 0.0473\n",
      "Epoch 21 Batch 1500 Loss 0.1145\n",
      "Epoch 21 Batch 1600 Loss 0.1116\n",
      "Epoch 21 Batch 1700 Loss 0.0802\n",
      "Epoch 21 Batch 1800 Loss 0.0711\n",
      "Epoch 21 Batch 1900 Loss 0.1196\n",
      "Epoch 21 Batch 2000 Loss 0.1380\n",
      "Epoch 21 Batch 2100 Loss 0.1037\n",
      "Epoch 21 Batch 2200 Loss 0.1253\n",
      "Epoch 21 Batch 2300 Loss 0.1044\n",
      "Epoch 21 Batch 2400 Loss 0.0745\n",
      "Epoch 21 Batch 2500 Loss 0.0806\n",
      "Epoch 21 Batch 2600 Loss 0.1441\n",
      "Epoch 21 Batch 2700 Loss 0.1015\n",
      "Epoch 21 Batch 2800 Loss 0.1514\n",
      "Epoch 21 Batch 2900 Loss 0.1122\n",
      "Epoch 21 Batch 3000 Loss 0.1188\n",
      "Epoch 21 Batch 3100 Loss 0.1112\n",
      "Epoch 21 Batch 3200 Loss 0.0945\n",
      "Epoch 21 Batch 3300 Loss 0.1162\n",
      "Epoch 21 Batch 3400 Loss 0.1032\n",
      "Epoch 21 Batch 3500 Loss 0.0912\n",
      "Epoch 21 Batch 3600 Loss 0.1487\n",
      "Epoch 21 Batch 3700 Loss 0.1518\n",
      "Epoch 21 Loss 0.0987\n",
      "Time taken for 1 epoch 107.54280185699463 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 0.0808\n",
      "Epoch 22 Batch 100 Loss 0.0729\n",
      "Epoch 22 Batch 200 Loss 0.0863\n",
      "Epoch 22 Batch 300 Loss 0.0986\n",
      "Epoch 22 Batch 400 Loss 0.0739\n",
      "Epoch 22 Batch 500 Loss 0.0783\n",
      "Epoch 22 Batch 600 Loss 0.0537\n",
      "Epoch 22 Batch 700 Loss 0.0823\n",
      "Epoch 22 Batch 800 Loss 0.0925\n",
      "Epoch 22 Batch 900 Loss 0.0857\n",
      "Epoch 22 Batch 1000 Loss 0.0817\n",
      "Epoch 22 Batch 1100 Loss 0.0676\n",
      "Epoch 22 Batch 1200 Loss 0.0802\n",
      "Epoch 22 Batch 1300 Loss 0.1136\n",
      "Epoch 22 Batch 1400 Loss 0.0620\n",
      "Epoch 22 Batch 1500 Loss 0.0775\n",
      "Epoch 22 Batch 1600 Loss 0.1463\n",
      "Epoch 22 Batch 1700 Loss 0.0845\n",
      "Epoch 22 Batch 1800 Loss 0.1450\n",
      "Epoch 22 Batch 1900 Loss 0.0916\n",
      "Epoch 22 Batch 2000 Loss 0.1111\n",
      "Epoch 22 Batch 2100 Loss 0.1239\n",
      "Epoch 22 Batch 2200 Loss 0.1251\n",
      "Epoch 22 Batch 2300 Loss 0.1273\n",
      "Epoch 22 Batch 2400 Loss 0.1123\n",
      "Epoch 22 Batch 2500 Loss 0.0827\n",
      "Epoch 22 Batch 2600 Loss 0.1444\n",
      "Epoch 22 Batch 2700 Loss 0.0849\n",
      "Epoch 22 Batch 2800 Loss 0.1108\n",
      "Epoch 22 Batch 2900 Loss 0.1508\n",
      "Epoch 22 Batch 3000 Loss 0.0979\n",
      "Epoch 22 Batch 3100 Loss 0.0951\n",
      "Epoch 22 Batch 3200 Loss 0.1466\n",
      "Epoch 22 Batch 3300 Loss 0.0793\n",
      "Epoch 22 Batch 3400 Loss 0.0994\n",
      "Epoch 22 Batch 3500 Loss 0.1405\n",
      "Epoch 22 Batch 3600 Loss 0.1344\n",
      "Epoch 22 Batch 3700 Loss 0.1584\n",
      "Epoch 22 Loss 0.0988\n",
      "Time taken for 1 epoch 110.18407821655273 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 0.0798\n",
      "Epoch 23 Batch 100 Loss 0.0977\n",
      "Epoch 23 Batch 200 Loss 0.0876\n",
      "Epoch 23 Batch 300 Loss 0.0933\n",
      "Epoch 23 Batch 400 Loss 0.0889\n",
      "Epoch 23 Batch 500 Loss 0.0393\n",
      "Epoch 23 Batch 600 Loss 0.0961\n",
      "Epoch 23 Batch 700 Loss 0.1056\n",
      "Epoch 23 Batch 800 Loss 0.0778\n",
      "Epoch 23 Batch 900 Loss 0.0915\n",
      "Epoch 23 Batch 1000 Loss 0.0780\n",
      "Epoch 23 Batch 1100 Loss 0.0723\n",
      "Epoch 23 Batch 1200 Loss 0.1440\n",
      "Epoch 23 Batch 1300 Loss 0.1138\n",
      "Epoch 23 Batch 1400 Loss 0.0811\n",
      "Epoch 23 Batch 1500 Loss 0.1108\n",
      "Epoch 23 Batch 1600 Loss 0.1127\n",
      "Epoch 23 Batch 1700 Loss 0.0965\n",
      "Epoch 23 Batch 1800 Loss 0.1353\n",
      "Epoch 23 Batch 1900 Loss 0.1184\n",
      "Epoch 23 Batch 2000 Loss 0.0580\n",
      "Epoch 23 Batch 2100 Loss 0.0879\n",
      "Epoch 23 Batch 2200 Loss 0.0895\n",
      "Epoch 23 Batch 2300 Loss 0.1153\n",
      "Epoch 23 Batch 2400 Loss 0.1489\n",
      "Epoch 23 Batch 2500 Loss 0.1238\n",
      "Epoch 23 Batch 2600 Loss 0.1079\n",
      "Epoch 23 Batch 2700 Loss 0.0780\n",
      "Epoch 23 Batch 2800 Loss 0.1012\n",
      "Epoch 23 Batch 2900 Loss 0.1232\n",
      "Epoch 23 Batch 3000 Loss 0.1061\n",
      "Epoch 23 Batch 3100 Loss 0.1163\n",
      "Epoch 23 Batch 3200 Loss 0.1251\n",
      "Epoch 23 Batch 3300 Loss 0.1148\n",
      "Epoch 23 Batch 3400 Loss 0.0819\n",
      "Epoch 23 Batch 3500 Loss 0.1258\n",
      "Epoch 23 Batch 3600 Loss 0.1533\n",
      "Epoch 23 Batch 3700 Loss 0.1163\n",
      "Epoch 23 Loss 0.0986\n",
      "Time taken for 1 epoch 107.62707304954529 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.0973\n",
      "Epoch 24 Batch 100 Loss 0.0935\n",
      "Epoch 24 Batch 200 Loss 0.0844\n",
      "Epoch 24 Batch 300 Loss 0.0747\n",
      "Epoch 24 Batch 400 Loss 0.0637\n",
      "Epoch 24 Batch 500 Loss 0.0553\n",
      "Epoch 24 Batch 600 Loss 0.0834\n",
      "Epoch 24 Batch 700 Loss 0.1010\n",
      "Epoch 24 Batch 800 Loss 0.0550\n",
      "Epoch 24 Batch 900 Loss 0.1009\n",
      "Epoch 24 Batch 1000 Loss 0.0947\n",
      "Epoch 24 Batch 1100 Loss 0.0778\n",
      "Epoch 24 Batch 1200 Loss 0.1263\n",
      "Epoch 24 Batch 1300 Loss 0.0514\n",
      "Epoch 24 Batch 1400 Loss 0.0989\n",
      "Epoch 24 Batch 1500 Loss 0.1300\n",
      "Epoch 24 Batch 1600 Loss 0.1026\n",
      "Epoch 24 Batch 1700 Loss 0.0844\n",
      "Epoch 24 Batch 1800 Loss 0.1033\n",
      "Epoch 24 Batch 1900 Loss 0.1177\n",
      "Epoch 24 Batch 2000 Loss 0.0951\n",
      "Epoch 24 Batch 2100 Loss 0.0911\n",
      "Epoch 24 Batch 2200 Loss 0.0656\n",
      "Epoch 24 Batch 2300 Loss 0.1037\n",
      "Epoch 24 Batch 2400 Loss 0.0853\n",
      "Epoch 24 Batch 2500 Loss 0.0713\n",
      "Epoch 24 Batch 2600 Loss 0.1050\n",
      "Epoch 24 Batch 2700 Loss 0.1474\n",
      "Epoch 24 Batch 2800 Loss 0.0970\n",
      "Epoch 24 Batch 2900 Loss 0.1168\n",
      "Epoch 24 Batch 3000 Loss 0.1581\n",
      "Epoch 24 Batch 3100 Loss 0.1014\n",
      "Epoch 24 Batch 3200 Loss 0.0968\n",
      "Epoch 24 Batch 3300 Loss 0.1031\n",
      "Epoch 24 Batch 3400 Loss 0.1535\n",
      "Epoch 24 Batch 3500 Loss 0.1443\n",
      "Epoch 24 Batch 3600 Loss 0.1292\n",
      "Epoch 24 Batch 3700 Loss 0.1369\n",
      "Epoch 24 Loss 0.0989\n",
      "Time taken for 1 epoch 110.08098292350769 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.0955\n",
      "Epoch 25 Batch 100 Loss 0.0763\n",
      "Epoch 25 Batch 200 Loss 0.0698\n",
      "Epoch 25 Batch 300 Loss 0.0895\n",
      "Epoch 25 Batch 400 Loss 0.0659\n",
      "Epoch 25 Batch 500 Loss 0.0484\n",
      "Epoch 25 Batch 600 Loss 0.0772\n",
      "Epoch 25 Batch 700 Loss 0.0840\n",
      "Epoch 25 Batch 800 Loss 0.0628\n",
      "Epoch 25 Batch 900 Loss 0.1010\n",
      "Epoch 25 Batch 1000 Loss 0.0636\n",
      "Epoch 25 Batch 1100 Loss 0.0586\n",
      "Epoch 25 Batch 1200 Loss 0.0790\n",
      "Epoch 25 Batch 1300 Loss 0.1729\n",
      "Epoch 25 Batch 1400 Loss 0.0778\n",
      "Epoch 25 Batch 1500 Loss 0.1091\n",
      "Epoch 25 Batch 1600 Loss 0.1236\n",
      "Epoch 25 Batch 1700 Loss 0.1114\n",
      "Epoch 25 Batch 1800 Loss 0.1073\n",
      "Epoch 25 Batch 1900 Loss 0.0931\n",
      "Epoch 25 Batch 2000 Loss 0.1141\n",
      "Epoch 25 Batch 2100 Loss 0.0633\n",
      "Epoch 25 Batch 2200 Loss 0.1115\n",
      "Epoch 25 Batch 2300 Loss 0.0758\n",
      "Epoch 25 Batch 2400 Loss 0.0991\n",
      "Epoch 25 Batch 2500 Loss 0.1250\n",
      "Epoch 25 Batch 2600 Loss 0.0986\n",
      "Epoch 25 Batch 2700 Loss 0.0817\n",
      "Epoch 25 Batch 2800 Loss 0.1371\n",
      "Epoch 25 Batch 2900 Loss 0.0863\n",
      "Epoch 25 Batch 3000 Loss 0.1053\n",
      "Epoch 25 Batch 3100 Loss 0.1278\n",
      "Epoch 25 Batch 3200 Loss 0.1014\n",
      "Epoch 25 Batch 3300 Loss 0.1271\n",
      "Epoch 25 Batch 3400 Loss 0.1222\n",
      "Epoch 25 Batch 3500 Loss 0.1664\n",
      "Epoch 25 Batch 3600 Loss 0.1333\n",
      "Epoch 25 Batch 3700 Loss 0.1282\n",
      "Epoch 25 Loss 0.0993\n",
      "Time taken for 1 epoch 107.80641174316406 sec\n",
      "\n",
      "Epoch 26 Batch 0 Loss 0.0970\n",
      "Epoch 26 Batch 100 Loss 0.1100\n",
      "Epoch 26 Batch 200 Loss 0.0761\n",
      "Epoch 26 Batch 300 Loss 0.0860\n",
      "Epoch 26 Batch 400 Loss 0.0816\n",
      "Epoch 26 Batch 500 Loss 0.0614\n",
      "Epoch 26 Batch 600 Loss 0.1002\n",
      "Epoch 26 Batch 700 Loss 0.0727\n",
      "Epoch 26 Batch 800 Loss 0.0804\n",
      "Epoch 26 Batch 900 Loss 0.0700\n",
      "Epoch 26 Batch 1000 Loss 0.1063\n",
      "Epoch 26 Batch 1100 Loss 0.0630\n",
      "Epoch 26 Batch 1200 Loss 0.1274\n",
      "Epoch 26 Batch 1300 Loss 0.1057\n",
      "Epoch 26 Batch 1400 Loss 0.0846\n",
      "Epoch 26 Batch 1500 Loss 0.0783\n",
      "Epoch 26 Batch 1600 Loss 0.1181\n",
      "Epoch 26 Batch 1700 Loss 0.0831\n",
      "Epoch 26 Batch 1800 Loss 0.1086\n",
      "Epoch 26 Batch 1900 Loss 0.1084\n",
      "Epoch 26 Batch 2000 Loss 0.1109\n",
      "Epoch 26 Batch 2100 Loss 0.1150\n",
      "Epoch 26 Batch 2200 Loss 0.0690\n",
      "Epoch 26 Batch 2300 Loss 0.1430\n",
      "Epoch 26 Batch 2400 Loss 0.1350\n",
      "Epoch 26 Batch 2500 Loss 0.0947\n",
      "Epoch 26 Batch 2600 Loss 0.0765\n",
      "Epoch 26 Batch 2700 Loss 0.1455\n",
      "Epoch 26 Batch 2800 Loss 0.0996\n",
      "Epoch 26 Batch 2900 Loss 0.0763\n",
      "Epoch 26 Batch 3000 Loss 0.1445\n",
      "Epoch 26 Batch 3100 Loss 0.1038\n",
      "Epoch 26 Batch 3200 Loss 0.1233\n",
      "Epoch 26 Batch 3300 Loss 0.1858\n",
      "Epoch 26 Batch 3400 Loss 0.0893\n",
      "Epoch 26 Batch 3500 Loss 0.1140\n",
      "Epoch 26 Batch 3600 Loss 0.1345\n",
      "Epoch 26 Batch 3700 Loss 0.1437\n",
      "Epoch 26 Loss 0.1001\n",
      "Time taken for 1 epoch 109.99800539016724 sec\n",
      "\n",
      "Epoch 27 Batch 0 Loss 0.0517\n",
      "Epoch 27 Batch 100 Loss 0.0606\n",
      "Epoch 27 Batch 200 Loss 0.0633\n",
      "Epoch 27 Batch 300 Loss 0.1037\n",
      "Epoch 27 Batch 400 Loss 0.0657\n",
      "Epoch 27 Batch 500 Loss 0.0952\n",
      "Epoch 27 Batch 600 Loss 0.0584\n",
      "Epoch 27 Batch 700 Loss 0.1116\n",
      "Epoch 27 Batch 800 Loss 0.1066\n",
      "Epoch 27 Batch 900 Loss 0.0955\n",
      "Epoch 27 Batch 1000 Loss 0.0875\n",
      "Epoch 27 Batch 1100 Loss 0.1152\n",
      "Epoch 27 Batch 1200 Loss 0.0724\n",
      "Epoch 27 Batch 1300 Loss 0.0854\n",
      "Epoch 27 Batch 1400 Loss 0.0860\n",
      "Epoch 27 Batch 1500 Loss 0.1021\n",
      "Epoch 27 Batch 1600 Loss 0.0984\n",
      "Epoch 27 Batch 1700 Loss 0.1145\n",
      "Epoch 27 Batch 1800 Loss 0.1150\n",
      "Epoch 27 Batch 1900 Loss 0.1220\n",
      "Epoch 27 Batch 2000 Loss 0.1099\n",
      "Epoch 27 Batch 2100 Loss 0.1055\n",
      "Epoch 27 Batch 2200 Loss 0.1031\n",
      "Epoch 27 Batch 2300 Loss 0.1166\n",
      "Epoch 27 Batch 2400 Loss 0.0868\n",
      "Epoch 27 Batch 2500 Loss 0.0774\n",
      "Epoch 27 Batch 2600 Loss 0.1292\n",
      "Epoch 27 Batch 2700 Loss 0.1399\n",
      "Epoch 27 Batch 2800 Loss 0.1172\n",
      "Epoch 27 Batch 2900 Loss 0.1283\n",
      "Epoch 27 Batch 3000 Loss 0.1064\n",
      "Epoch 27 Batch 3100 Loss 0.1460\n",
      "Epoch 27 Batch 3200 Loss 0.1482\n",
      "Epoch 27 Batch 3300 Loss 0.0967\n",
      "Epoch 27 Batch 3400 Loss 0.1126\n",
      "Epoch 27 Batch 3500 Loss 0.1176\n",
      "Epoch 27 Batch 3600 Loss 0.0916\n",
      "Epoch 27 Batch 3700 Loss 0.1635\n",
      "Epoch 27 Loss 0.1007\n",
      "Time taken for 1 epoch 107.50869727134705 sec\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.0894\n",
      "Epoch 28 Batch 100 Loss 0.0966\n",
      "Epoch 28 Batch 200 Loss 0.1257\n",
      "Epoch 28 Batch 300 Loss 0.1024\n",
      "Epoch 28 Batch 400 Loss 0.0955\n",
      "Epoch 28 Batch 500 Loss 0.0591\n",
      "Epoch 28 Batch 600 Loss 0.0838\n",
      "Epoch 28 Batch 700 Loss 0.1034\n",
      "Epoch 28 Batch 800 Loss 0.1177\n",
      "Epoch 28 Batch 900 Loss 0.0576\n",
      "Epoch 28 Batch 1000 Loss 0.0749\n",
      "Epoch 28 Batch 1100 Loss 0.1004\n",
      "Epoch 28 Batch 1200 Loss 0.0783\n",
      "Epoch 28 Batch 1300 Loss 0.1333\n",
      "Epoch 28 Batch 1400 Loss 0.1151\n",
      "Epoch 28 Batch 1500 Loss 0.1481\n",
      "Epoch 28 Batch 1600 Loss 0.0712\n",
      "Epoch 28 Batch 1700 Loss 0.1442\n",
      "Epoch 28 Batch 1800 Loss 0.1022\n",
      "Epoch 28 Batch 1900 Loss 0.0757\n",
      "Epoch 28 Batch 2000 Loss 0.0898\n",
      "Epoch 28 Batch 2100 Loss 0.0855\n",
      "Epoch 28 Batch 2200 Loss 0.1002\n",
      "Epoch 28 Batch 2300 Loss 0.0820\n",
      "Epoch 28 Batch 2400 Loss 0.0860\n",
      "Epoch 28 Batch 2500 Loss 0.1115\n",
      "Epoch 28 Batch 2600 Loss 0.0969\n",
      "Epoch 28 Batch 2700 Loss 0.1065\n",
      "Epoch 28 Batch 2800 Loss 0.1155\n",
      "Epoch 28 Batch 2900 Loss 0.1080\n",
      "Epoch 28 Batch 3000 Loss 0.1144\n",
      "Epoch 28 Batch 3100 Loss 0.1037\n",
      "Epoch 28 Batch 3200 Loss 0.1187\n",
      "Epoch 28 Batch 3300 Loss 0.1501\n",
      "Epoch 28 Batch 3400 Loss 0.1399\n",
      "Epoch 28 Batch 3500 Loss 0.1405\n",
      "Epoch 28 Batch 3600 Loss 0.1091\n",
      "Epoch 28 Batch 3700 Loss 0.0842\n",
      "Epoch 28 Loss 0.1011\n",
      "Time taken for 1 epoch 109.95937728881836 sec\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.0967\n",
      "Epoch 29 Batch 100 Loss 0.1027\n",
      "Epoch 29 Batch 200 Loss 0.0706\n",
      "Epoch 29 Batch 300 Loss 0.0873\n",
      "Epoch 29 Batch 400 Loss 0.0902\n",
      "Epoch 29 Batch 500 Loss 0.0812\n",
      "Epoch 29 Batch 600 Loss 0.0607\n",
      "Epoch 29 Batch 700 Loss 0.0700\n",
      "Epoch 29 Batch 800 Loss 0.0674\n",
      "Epoch 29 Batch 900 Loss 0.0981\n",
      "Epoch 29 Batch 1000 Loss 0.0969\n",
      "Epoch 29 Batch 1100 Loss 0.0810\n",
      "Epoch 29 Batch 1200 Loss 0.0741\n",
      "Epoch 29 Batch 1300 Loss 0.0720\n",
      "Epoch 29 Batch 1400 Loss 0.1093\n",
      "Epoch 29 Batch 1500 Loss 0.0984\n",
      "Epoch 29 Batch 1600 Loss 0.0804\n",
      "Epoch 29 Batch 1700 Loss 0.1105\n",
      "Epoch 29 Batch 1800 Loss 0.1392\n",
      "Epoch 29 Batch 1900 Loss 0.1073\n",
      "Epoch 29 Batch 2000 Loss 0.0900\n",
      "Epoch 29 Batch 2100 Loss 0.0965\n",
      "Epoch 29 Batch 2200 Loss 0.0945\n",
      "Epoch 29 Batch 2300 Loss 0.1365\n",
      "Epoch 29 Batch 2400 Loss 0.0850\n",
      "Epoch 29 Batch 2500 Loss 0.0909\n",
      "Epoch 29 Batch 2600 Loss 0.1273\n",
      "Epoch 29 Batch 2700 Loss 0.1234\n",
      "Epoch 29 Batch 2800 Loss 0.1148\n",
      "Epoch 29 Batch 2900 Loss 0.0958\n",
      "Epoch 29 Batch 3000 Loss 0.1240\n",
      "Epoch 29 Batch 3100 Loss 0.1284\n",
      "Epoch 29 Batch 3200 Loss 0.1358\n",
      "Epoch 29 Batch 3300 Loss 0.1189\n",
      "Epoch 29 Batch 3400 Loss 0.1233\n",
      "Epoch 29 Batch 3500 Loss 0.0967\n",
      "Epoch 29 Batch 3600 Loss 0.1228\n",
      "Epoch 29 Batch 3700 Loss 0.1270\n",
      "Epoch 29 Loss 0.1019\n",
      "Time taken for 1 epoch 107.63013505935669 sec\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.1086\n",
      "Epoch 30 Batch 100 Loss 0.0616\n",
      "Epoch 30 Batch 200 Loss 0.0871\n",
      "Epoch 30 Batch 300 Loss 0.0682\n",
      "Epoch 30 Batch 400 Loss 0.0742\n",
      "Epoch 30 Batch 500 Loss 0.0946\n",
      "Epoch 30 Batch 600 Loss 0.0783\n",
      "Epoch 30 Batch 700 Loss 0.0862\n",
      "Epoch 30 Batch 800 Loss 0.0735\n",
      "Epoch 30 Batch 900 Loss 0.1408\n",
      "Epoch 30 Batch 1000 Loss 0.0813\n",
      "Epoch 30 Batch 1100 Loss 0.1202\n",
      "Epoch 30 Batch 1200 Loss 0.0916\n",
      "Epoch 30 Batch 1300 Loss 0.1096\n",
      "Epoch 30 Batch 1400 Loss 0.1050\n",
      "Epoch 30 Batch 1500 Loss 0.0854\n",
      "Epoch 30 Batch 1600 Loss 0.1216\n",
      "Epoch 30 Batch 1700 Loss 0.1263\n",
      "Epoch 30 Batch 1800 Loss 0.1132\n",
      "Epoch 30 Batch 1900 Loss 0.0951\n",
      "Epoch 30 Batch 2000 Loss 0.0838\n",
      "Epoch 30 Batch 2100 Loss 0.1767\n",
      "Epoch 30 Batch 2200 Loss 0.1021\n",
      "Epoch 30 Batch 2300 Loss 0.0814\n",
      "Epoch 30 Batch 2400 Loss 0.1020\n",
      "Epoch 30 Batch 2500 Loss 0.1308\n",
      "Epoch 30 Batch 2600 Loss 0.1265\n",
      "Epoch 30 Batch 2700 Loss 0.1092\n",
      "Epoch 30 Batch 2800 Loss 0.0790\n",
      "Epoch 30 Batch 2900 Loss 0.0910\n",
      "Epoch 30 Batch 3000 Loss 0.0824\n",
      "Epoch 30 Batch 3100 Loss 0.0949\n",
      "Epoch 30 Batch 3200 Loss 0.0943\n",
      "Epoch 30 Batch 3300 Loss 0.1319\n",
      "Epoch 30 Batch 3400 Loss 0.1230\n",
      "Epoch 30 Batch 3500 Loss 0.1766\n",
      "Epoch 30 Batch 3600 Loss 0.0976\n",
      "Epoch 30 Batch 3700 Loss 0.1061\n",
      "Epoch 30 Loss 0.1026\n",
      "Time taken for 1 epoch 109.75093913078308 sec\n",
      "\n",
      "Epoch 31 Batch 0 Loss 0.0861\n",
      "Epoch 31 Batch 100 Loss 0.1291\n",
      "Epoch 31 Batch 200 Loss 0.0597\n",
      "Epoch 31 Batch 300 Loss 0.0587\n",
      "Epoch 31 Batch 400 Loss 0.0629\n",
      "Epoch 31 Batch 500 Loss 0.1046\n",
      "Epoch 31 Batch 600 Loss 0.0853\n",
      "Epoch 31 Batch 700 Loss 0.1192\n",
      "Epoch 31 Batch 800 Loss 0.0741\n",
      "Epoch 31 Batch 900 Loss 0.0649\n",
      "Epoch 31 Batch 1000 Loss 0.0954\n",
      "Epoch 31 Batch 1100 Loss 0.0744\n",
      "Epoch 31 Batch 1200 Loss 0.1050\n",
      "Epoch 31 Batch 1300 Loss 0.1271\n",
      "Epoch 31 Batch 1400 Loss 0.1196\n",
      "Epoch 31 Batch 1500 Loss 0.0768\n",
      "Epoch 31 Batch 1600 Loss 0.1165\n",
      "Epoch 31 Batch 1700 Loss 0.0847\n",
      "Epoch 31 Batch 1800 Loss 0.0958\n",
      "Epoch 31 Batch 1900 Loss 0.0887\n",
      "Epoch 31 Batch 2000 Loss 0.1495\n",
      "Epoch 31 Batch 2100 Loss 0.1152\n",
      "Epoch 31 Batch 2200 Loss 0.0975\n",
      "Epoch 31 Batch 2300 Loss 0.0911\n",
      "Epoch 31 Batch 2400 Loss 0.1079\n",
      "Epoch 31 Batch 2500 Loss 0.1032\n",
      "Epoch 31 Batch 2600 Loss 0.1279\n",
      "Epoch 31 Batch 2700 Loss 0.1485\n",
      "Epoch 31 Batch 2800 Loss 0.0984\n",
      "Epoch 31 Batch 2900 Loss 0.0887\n",
      "Epoch 31 Batch 3000 Loss 0.0880\n",
      "Epoch 31 Batch 3100 Loss 0.1492\n",
      "Epoch 31 Batch 3200 Loss 0.1134\n",
      "Epoch 31 Batch 3300 Loss 0.1309\n",
      "Epoch 31 Batch 3400 Loss 0.0864\n",
      "Epoch 31 Batch 3500 Loss 0.0963\n",
      "Epoch 31 Batch 3600 Loss 0.1195\n",
      "Epoch 31 Batch 3700 Loss 0.0913\n",
      "Epoch 31 Loss 0.1042\n",
      "Time taken for 1 epoch 107.81397604942322 sec\n",
      "\n",
      "Epoch 32 Batch 0 Loss 0.0750\n",
      "Epoch 32 Batch 100 Loss 0.0870\n",
      "Epoch 32 Batch 200 Loss 0.0550\n",
      "Epoch 32 Batch 300 Loss 0.0598\n",
      "Epoch 32 Batch 400 Loss 0.0675\n",
      "Epoch 32 Batch 500 Loss 0.1078\n",
      "Epoch 32 Batch 600 Loss 0.0700\n",
      "Epoch 32 Batch 700 Loss 0.0902\n",
      "Epoch 32 Batch 800 Loss 0.1519\n",
      "Epoch 32 Batch 900 Loss 0.1015\n",
      "Epoch 32 Batch 1000 Loss 0.0755\n",
      "Epoch 32 Batch 1100 Loss 0.0593\n",
      "Epoch 32 Batch 1200 Loss 0.0580\n",
      "Epoch 32 Batch 1300 Loss 0.0977\n",
      "Epoch 32 Batch 1400 Loss 0.0894\n",
      "Epoch 32 Batch 1500 Loss 0.1241\n",
      "Epoch 32 Batch 1600 Loss 0.0741\n",
      "Epoch 32 Batch 1700 Loss 0.1091\n",
      "Epoch 32 Batch 1800 Loss 0.0653\n",
      "Epoch 32 Batch 1900 Loss 0.1186\n",
      "Epoch 32 Batch 2000 Loss 0.1093\n",
      "Epoch 32 Batch 2100 Loss 0.1197\n",
      "Epoch 32 Batch 2200 Loss 0.1553\n",
      "Epoch 32 Batch 2300 Loss 0.1147\n",
      "Epoch 32 Batch 2400 Loss 0.0729\n",
      "Epoch 32 Batch 2500 Loss 0.0852\n",
      "Epoch 32 Batch 2600 Loss 0.1080\n",
      "Epoch 32 Batch 2700 Loss 0.1297\n",
      "Epoch 32 Batch 2800 Loss 0.1623\n",
      "Epoch 32 Batch 2900 Loss 0.1469\n",
      "Epoch 32 Batch 3000 Loss 0.0788\n",
      "Epoch 32 Batch 3100 Loss 0.1203\n",
      "Epoch 32 Batch 3200 Loss 0.1095\n",
      "Epoch 32 Batch 3300 Loss 0.1301\n",
      "Epoch 32 Batch 3400 Loss 0.1627\n",
      "Epoch 32 Batch 3500 Loss 0.1727\n",
      "Epoch 32 Batch 3600 Loss 0.1324\n",
      "Epoch 32 Batch 3700 Loss 0.1843\n",
      "Epoch 32 Loss 0.1050\n",
      "Time taken for 1 epoch 109.85430264472961 sec\n",
      "\n",
      "Epoch 33 Batch 0 Loss 0.1015\n",
      "Epoch 33 Batch 100 Loss 0.0674\n",
      "Epoch 33 Batch 200 Loss 0.1099\n",
      "Epoch 33 Batch 300 Loss 0.0946\n",
      "Epoch 33 Batch 400 Loss 0.1130\n",
      "Epoch 33 Batch 500 Loss 0.1111\n",
      "Epoch 33 Batch 600 Loss 0.0805\n",
      "Epoch 33 Batch 700 Loss 0.1148\n",
      "Epoch 33 Batch 800 Loss 0.0847\n",
      "Epoch 33 Batch 900 Loss 0.0906\n",
      "Epoch 33 Batch 1000 Loss 0.0993\n",
      "Epoch 33 Batch 1100 Loss 0.0914\n",
      "Epoch 33 Batch 1200 Loss 0.0980\n",
      "Epoch 33 Batch 1300 Loss 0.0763\n",
      "Epoch 33 Batch 1400 Loss 0.0624\n",
      "Epoch 33 Batch 1500 Loss 0.1233\n",
      "Epoch 33 Batch 1600 Loss 0.1455\n",
      "Epoch 33 Batch 1700 Loss 0.0904\n",
      "Epoch 33 Batch 1800 Loss 0.0931\n",
      "Epoch 33 Batch 1900 Loss 0.1516\n",
      "Epoch 33 Batch 2000 Loss 0.1074\n",
      "Epoch 33 Batch 2100 Loss 0.1074\n",
      "Epoch 33 Batch 2200 Loss 0.0959\n",
      "Epoch 33 Batch 2300 Loss 0.0850\n",
      "Epoch 33 Batch 2400 Loss 0.1084\n",
      "Epoch 33 Batch 2500 Loss 0.1394\n",
      "Epoch 33 Batch 2600 Loss 0.0950\n",
      "Epoch 33 Batch 2700 Loss 0.0976\n",
      "Epoch 33 Batch 2800 Loss 0.1072\n",
      "Epoch 33 Batch 2900 Loss 0.0901\n",
      "Epoch 33 Batch 3000 Loss 0.1132\n",
      "Epoch 33 Batch 3100 Loss 0.0945\n",
      "Epoch 33 Batch 3200 Loss 0.1001\n",
      "Epoch 33 Batch 3300 Loss 0.0875\n",
      "Epoch 33 Batch 3400 Loss 0.1756\n",
      "Epoch 33 Batch 3500 Loss 0.1169\n",
      "Epoch 33 Batch 3600 Loss 0.1329\n",
      "Epoch 33 Batch 3700 Loss 0.0977\n",
      "Epoch 33 Loss 0.1052\n",
      "Time taken for 1 epoch 107.6378607749939 sec\n",
      "\n",
      "Epoch 34 Batch 0 Loss 0.0987\n",
      "Epoch 34 Batch 100 Loss 0.0637\n",
      "Epoch 34 Batch 200 Loss 0.0594\n",
      "Epoch 34 Batch 300 Loss 0.0810\n",
      "Epoch 34 Batch 400 Loss 0.0631\n",
      "Epoch 34 Batch 500 Loss 0.0758\n",
      "Epoch 34 Batch 600 Loss 0.0859\n",
      "Epoch 34 Batch 700 Loss 0.0891\n",
      "Epoch 34 Batch 800 Loss 0.0940\n",
      "Epoch 34 Batch 900 Loss 0.1395\n",
      "Epoch 34 Batch 1000 Loss 0.0942\n",
      "Epoch 34 Batch 1100 Loss 0.0843\n",
      "Epoch 34 Batch 1200 Loss 0.1301\n",
      "Epoch 34 Batch 1300 Loss 0.1552\n",
      "Epoch 34 Batch 1400 Loss 0.0688\n",
      "Epoch 34 Batch 1500 Loss 0.0803\n",
      "Epoch 34 Batch 1600 Loss 0.0715\n",
      "Epoch 34 Batch 1700 Loss 0.0914\n",
      "Epoch 34 Batch 1800 Loss 0.0824\n",
      "Epoch 34 Batch 1900 Loss 0.1820\n",
      "Epoch 34 Batch 2000 Loss 0.1069\n",
      "Epoch 34 Batch 2100 Loss 0.1103\n",
      "Epoch 34 Batch 2200 Loss 0.1403\n",
      "Epoch 34 Batch 2300 Loss 0.0829\n",
      "Epoch 34 Batch 2400 Loss 0.1050\n",
      "Epoch 34 Batch 2500 Loss 0.1262\n",
      "Epoch 34 Batch 2600 Loss 0.1237\n",
      "Epoch 34 Batch 2700 Loss 0.1032\n",
      "Epoch 34 Batch 2800 Loss 0.1335\n",
      "Epoch 34 Batch 2900 Loss 0.1215\n",
      "Epoch 34 Batch 3000 Loss 0.1351\n",
      "Epoch 34 Batch 3100 Loss 0.1463\n",
      "Epoch 34 Batch 3200 Loss 0.1377\n",
      "Epoch 34 Batch 3300 Loss 0.0975\n",
      "Epoch 34 Batch 3400 Loss 0.1051\n",
      "Epoch 34 Batch 3500 Loss 0.1650\n",
      "Epoch 34 Batch 3600 Loss 0.0773\n",
      "Epoch 34 Batch 3700 Loss 0.1128\n",
      "Epoch 34 Loss 0.1052\n",
      "Time taken for 1 epoch 109.71600151062012 sec\n",
      "\n",
      "Epoch 35 Batch 0 Loss 0.1112\n",
      "Epoch 35 Batch 100 Loss 0.0684\n",
      "Epoch 35 Batch 200 Loss 0.0878\n",
      "Epoch 35 Batch 300 Loss 0.0843\n",
      "Epoch 35 Batch 400 Loss 0.0897\n",
      "Epoch 35 Batch 500 Loss 0.0830\n",
      "Epoch 35 Batch 600 Loss 0.0791\n",
      "Epoch 35 Batch 700 Loss 0.1163\n",
      "Epoch 35 Batch 800 Loss 0.0823\n",
      "Epoch 35 Batch 900 Loss 0.0933\n",
      "Epoch 35 Batch 1000 Loss 0.0971\n",
      "Epoch 35 Batch 1100 Loss 0.0772\n",
      "Epoch 35 Batch 1200 Loss 0.0997\n",
      "Epoch 35 Batch 1300 Loss 0.0936\n",
      "Epoch 35 Batch 1400 Loss 0.1024\n",
      "Epoch 35 Batch 1500 Loss 0.1251\n",
      "Epoch 35 Batch 1600 Loss 0.1655\n",
      "Epoch 35 Batch 1700 Loss 0.0949\n",
      "Epoch 35 Batch 1800 Loss 0.0976\n",
      "Epoch 35 Batch 1900 Loss 0.0839\n",
      "Epoch 35 Batch 2000 Loss 0.1091\n",
      "Epoch 35 Batch 2100 Loss 0.0850\n",
      "Epoch 35 Batch 2200 Loss 0.0838\n",
      "Epoch 35 Batch 2300 Loss 0.1003\n",
      "Epoch 35 Batch 2400 Loss 0.0880\n",
      "Epoch 35 Batch 2500 Loss 0.0711\n",
      "Epoch 35 Batch 2600 Loss 0.1032\n",
      "Epoch 35 Batch 2700 Loss 0.1324\n",
      "Epoch 35 Batch 2800 Loss 0.1273\n",
      "Epoch 35 Batch 2900 Loss 0.1285\n",
      "Epoch 35 Batch 3000 Loss 0.0935\n",
      "Epoch 35 Batch 3100 Loss 0.0928\n",
      "Epoch 35 Batch 3200 Loss 0.1187\n",
      "Epoch 35 Batch 3300 Loss 0.1101\n",
      "Epoch 35 Batch 3400 Loss 0.1352\n",
      "Epoch 35 Batch 3500 Loss 0.1046\n",
      "Epoch 35 Batch 3600 Loss 0.1116\n",
      "Epoch 35 Batch 3700 Loss 0.0990\n",
      "Epoch 35 Loss 0.1062\n",
      "Time taken for 1 epoch 107.82676672935486 sec\n",
      "\n",
      "Epoch 36 Batch 0 Loss 0.1229\n",
      "Epoch 36 Batch 100 Loss 0.0719\n",
      "Epoch 36 Batch 200 Loss 0.0972\n",
      "Epoch 36 Batch 300 Loss 0.0956\n",
      "Epoch 36 Batch 400 Loss 0.0808\n",
      "Epoch 36 Batch 500 Loss 0.0954\n",
      "Epoch 36 Batch 600 Loss 0.1108\n",
      "Epoch 36 Batch 700 Loss 0.1257\n",
      "Epoch 36 Batch 800 Loss 0.0956\n",
      "Epoch 36 Batch 900 Loss 0.0943\n",
      "Epoch 36 Batch 1000 Loss 0.0757\n",
      "Epoch 36 Batch 1100 Loss 0.1087\n",
      "Epoch 36 Batch 1200 Loss 0.0872\n",
      "Epoch 36 Batch 1300 Loss 0.1636\n",
      "Epoch 36 Batch 1400 Loss 0.1654\n",
      "Epoch 36 Batch 1500 Loss 0.1215\n",
      "Epoch 36 Batch 1600 Loss 0.0863\n",
      "Epoch 36 Batch 1700 Loss 0.1318\n",
      "Epoch 36 Batch 1800 Loss 0.0889\n",
      "Epoch 36 Batch 1900 Loss 0.1033\n",
      "Epoch 36 Batch 2000 Loss 0.1078\n",
      "Epoch 36 Batch 2100 Loss 0.1108\n",
      "Epoch 36 Batch 2200 Loss 0.0937\n",
      "Epoch 36 Batch 2300 Loss 0.1116\n",
      "Epoch 36 Batch 2400 Loss 0.0802\n",
      "Epoch 36 Batch 2500 Loss 0.1256\n",
      "Epoch 36 Batch 2600 Loss 0.0733\n",
      "Epoch 36 Batch 2700 Loss 0.1046\n",
      "Epoch 36 Batch 2800 Loss 0.1233\n",
      "Epoch 36 Batch 2900 Loss 0.1426\n",
      "Epoch 36 Batch 3000 Loss 0.1522\n",
      "Epoch 36 Batch 3100 Loss 0.0978\n",
      "Epoch 36 Batch 3200 Loss 0.1027\n",
      "Epoch 36 Batch 3300 Loss 0.1519\n",
      "Epoch 36 Batch 3400 Loss 0.1090\n",
      "Epoch 36 Batch 3500 Loss 0.1034\n",
      "Epoch 36 Batch 3600 Loss 0.1593\n",
      "Epoch 36 Batch 3700 Loss 0.1247\n",
      "Epoch 36 Loss 0.1066\n",
      "Time taken for 1 epoch 109.81019949913025 sec\n",
      "\n",
      "Epoch 37 Batch 0 Loss 0.0856\n",
      "Epoch 37 Batch 100 Loss 0.1178\n",
      "Epoch 37 Batch 200 Loss 0.0914\n",
      "Epoch 37 Batch 300 Loss 0.0990\n",
      "Epoch 37 Batch 400 Loss 0.1098\n",
      "Epoch 37 Batch 500 Loss 0.1020\n",
      "Epoch 37 Batch 600 Loss 0.0937\n",
      "Epoch 37 Batch 700 Loss 0.1418\n",
      "Epoch 37 Batch 800 Loss 0.0951\n",
      "Epoch 37 Batch 900 Loss 0.0839\n",
      "Epoch 37 Batch 1000 Loss 0.0952\n",
      "Epoch 37 Batch 1100 Loss 0.1106\n",
      "Epoch 37 Batch 1200 Loss 0.0848\n",
      "Epoch 37 Batch 1300 Loss 0.0646\n",
      "Epoch 37 Batch 1400 Loss 0.0829\n",
      "Epoch 37 Batch 1500 Loss 0.1045\n",
      "Epoch 37 Batch 1600 Loss 0.1135\n",
      "Epoch 37 Batch 1700 Loss 0.0969\n",
      "Epoch 37 Batch 1800 Loss 0.1232\n",
      "Epoch 37 Batch 1900 Loss 0.1096\n",
      "Epoch 37 Batch 2000 Loss 0.0862\n",
      "Epoch 37 Batch 2100 Loss 0.0669\n",
      "Epoch 37 Batch 2200 Loss 0.1010\n",
      "Epoch 37 Batch 2300 Loss 0.1008\n",
      "Epoch 37 Batch 2400 Loss 0.0700\n",
      "Epoch 37 Batch 2500 Loss 0.1379\n",
      "Epoch 37 Batch 2600 Loss 0.1152\n",
      "Epoch 37 Batch 2700 Loss 0.1236\n",
      "Epoch 37 Batch 2800 Loss 0.1032\n",
      "Epoch 37 Batch 2900 Loss 0.1070\n",
      "Epoch 37 Batch 3000 Loss 0.1337\n",
      "Epoch 37 Batch 3100 Loss 0.1390\n",
      "Epoch 37 Batch 3200 Loss 0.1506\n",
      "Epoch 37 Batch 3300 Loss 0.0920\n",
      "Epoch 37 Batch 3400 Loss 0.1097\n",
      "Epoch 37 Batch 3500 Loss 0.1300\n",
      "Epoch 37 Batch 3600 Loss 0.1456\n",
      "Epoch 37 Batch 3700 Loss 0.1692\n",
      "Epoch 37 Loss 0.1084\n",
      "Time taken for 1 epoch 107.61716675758362 sec\n",
      "\n",
      "Epoch 38 Batch 0 Loss 0.0760\n",
      "Epoch 38 Batch 100 Loss 0.0754\n",
      "Epoch 38 Batch 200 Loss 0.0746\n",
      "Epoch 38 Batch 300 Loss 0.1120\n",
      "Epoch 38 Batch 400 Loss 0.0530\n",
      "Epoch 38 Batch 500 Loss 0.0727\n",
      "Epoch 38 Batch 600 Loss 0.1266\n",
      "Epoch 38 Batch 700 Loss 0.0883\n",
      "Epoch 38 Batch 800 Loss 0.1139\n",
      "Epoch 38 Batch 900 Loss 0.0833\n",
      "Epoch 38 Batch 1000 Loss 0.0832\n",
      "Epoch 38 Batch 1100 Loss 0.1499\n",
      "Epoch 38 Batch 1200 Loss 0.0937\n",
      "Epoch 38 Batch 1300 Loss 0.0653\n",
      "Epoch 38 Batch 1400 Loss 0.1305\n",
      "Epoch 38 Batch 1500 Loss 0.1001\n",
      "Epoch 38 Batch 1600 Loss 0.0911\n",
      "Epoch 38 Batch 1700 Loss 0.1237\n",
      "Epoch 38 Batch 1800 Loss 0.1067\n",
      "Epoch 38 Batch 1900 Loss 0.1340\n",
      "Epoch 38 Batch 2000 Loss 0.1588\n",
      "Epoch 38 Batch 2100 Loss 0.1102\n",
      "Epoch 38 Batch 2200 Loss 0.1013\n",
      "Epoch 38 Batch 2300 Loss 0.1078\n",
      "Epoch 38 Batch 2400 Loss 0.1145\n",
      "Epoch 38 Batch 2500 Loss 0.1196\n",
      "Epoch 38 Batch 2600 Loss 0.1282\n",
      "Epoch 38 Batch 2700 Loss 0.0948\n",
      "Epoch 38 Batch 2800 Loss 0.1333\n",
      "Epoch 38 Batch 2900 Loss 0.1051\n",
      "Epoch 38 Batch 3000 Loss 0.1020\n",
      "Epoch 38 Batch 3100 Loss 0.1141\n",
      "Epoch 38 Batch 3200 Loss 0.0839\n",
      "Epoch 38 Batch 3300 Loss 0.1446\n",
      "Epoch 38 Batch 3400 Loss 0.1011\n",
      "Epoch 38 Batch 3500 Loss 0.1385\n",
      "Epoch 38 Batch 3600 Loss 0.1008\n",
      "Epoch 38 Batch 3700 Loss 0.1604\n",
      "Epoch 38 Loss 0.1088\n",
      "Time taken for 1 epoch 110.05831170082092 sec\n",
      "\n",
      "Epoch 39 Batch 0 Loss 0.0637\n",
      "Epoch 39 Batch 100 Loss 0.0595\n",
      "Epoch 39 Batch 200 Loss 0.0689\n",
      "Epoch 39 Batch 300 Loss 0.0755\n",
      "Epoch 39 Batch 400 Loss 0.0785\n",
      "Epoch 39 Batch 500 Loss 0.0699\n",
      "Epoch 39 Batch 600 Loss 0.1139\n",
      "Epoch 39 Batch 700 Loss 0.0985\n",
      "Epoch 39 Batch 800 Loss 0.1230\n",
      "Epoch 39 Batch 900 Loss 0.0821\n",
      "Epoch 39 Batch 1000 Loss 0.1158\n",
      "Epoch 39 Batch 1100 Loss 0.1371\n",
      "Epoch 39 Batch 1200 Loss 0.1164\n",
      "Epoch 39 Batch 1300 Loss 0.0863\n",
      "Epoch 39 Batch 1400 Loss 0.0891\n",
      "Epoch 39 Batch 1500 Loss 0.1266\n",
      "Epoch 39 Batch 1600 Loss 0.1087\n",
      "Epoch 39 Batch 1700 Loss 0.0983\n",
      "Epoch 39 Batch 1800 Loss 0.0861\n",
      "Epoch 39 Batch 1900 Loss 0.0878\n",
      "Epoch 39 Batch 2000 Loss 0.0891\n",
      "Epoch 39 Batch 2100 Loss 0.1368\n",
      "Epoch 39 Batch 2200 Loss 0.1349\n",
      "Epoch 39 Batch 2300 Loss 0.0837\n",
      "Epoch 39 Batch 2400 Loss 0.0908\n",
      "Epoch 39 Batch 2500 Loss 0.1092\n",
      "Epoch 39 Batch 2600 Loss 0.0915\n",
      "Epoch 39 Batch 2700 Loss 0.0705\n",
      "Epoch 39 Batch 2800 Loss 0.1196\n",
      "Epoch 39 Batch 2900 Loss 0.1054\n",
      "Epoch 39 Batch 3000 Loss 0.1570\n",
      "Epoch 39 Batch 3100 Loss 0.1259\n",
      "Epoch 39 Batch 3200 Loss 0.1337\n",
      "Epoch 39 Batch 3300 Loss 0.1353\n",
      "Epoch 39 Batch 3400 Loss 0.1088\n",
      "Epoch 39 Batch 3500 Loss 0.1350\n",
      "Epoch 39 Batch 3600 Loss 0.1584\n",
      "Epoch 39 Batch 3700 Loss 0.1415\n",
      "Epoch 39 Loss 0.1092\n",
      "Time taken for 1 epoch 107.99594140052795 sec\n",
      "\n",
      "Epoch 40 Batch 0 Loss 0.1288\n",
      "Epoch 40 Batch 100 Loss 0.1140\n",
      "Epoch 40 Batch 200 Loss 0.1010\n",
      "Epoch 40 Batch 300 Loss 0.0906\n",
      "Epoch 40 Batch 400 Loss 0.1292\n",
      "Epoch 40 Batch 500 Loss 0.0928\n",
      "Epoch 40 Batch 600 Loss 0.1028\n",
      "Epoch 40 Batch 700 Loss 0.0896\n",
      "Epoch 40 Batch 800 Loss 0.1041\n",
      "Epoch 40 Batch 900 Loss 0.1020\n",
      "Epoch 40 Batch 1000 Loss 0.0699\n",
      "Epoch 40 Batch 1100 Loss 0.1464\n",
      "Epoch 40 Batch 1200 Loss 0.0895\n",
      "Epoch 40 Batch 1300 Loss 0.1027\n",
      "Epoch 40 Batch 1400 Loss 0.1423\n",
      "Epoch 40 Batch 1500 Loss 0.1335\n",
      "Epoch 40 Batch 1600 Loss 0.1349\n",
      "Epoch 40 Batch 1700 Loss 0.1474\n",
      "Epoch 40 Batch 1800 Loss 0.0917\n",
      "Epoch 40 Batch 1900 Loss 0.0961\n",
      "Epoch 40 Batch 2000 Loss 0.1431\n",
      "Epoch 40 Batch 2100 Loss 0.0918\n",
      "Epoch 40 Batch 2200 Loss 0.0880\n",
      "Epoch 40 Batch 2300 Loss 0.1231\n",
      "Epoch 40 Batch 2400 Loss 0.0828\n",
      "Epoch 40 Batch 2500 Loss 0.1088\n",
      "Epoch 40 Batch 2600 Loss 0.1834\n",
      "Epoch 40 Batch 2700 Loss 0.1601\n",
      "Epoch 40 Batch 2800 Loss 0.1416\n",
      "Epoch 40 Batch 2900 Loss 0.1439\n",
      "Epoch 40 Batch 3000 Loss 0.1033\n",
      "Epoch 40 Batch 3100 Loss 0.1149\n",
      "Epoch 40 Batch 3200 Loss 0.1660\n",
      "Epoch 40 Batch 3300 Loss 0.1240\n",
      "Epoch 40 Batch 3400 Loss 0.1261\n",
      "Epoch 40 Batch 3500 Loss 0.1086\n",
      "Epoch 40 Batch 3600 Loss 0.1570\n",
      "Epoch 40 Batch 3700 Loss 0.1505\n",
      "Epoch 40 Loss 0.1099\n",
      "Time taken for 1 epoch 110.25522255897522 sec\n",
      "\n",
      "Epoch 41 Batch 0 Loss 0.0960\n",
      "Epoch 41 Batch 100 Loss 0.0728\n",
      "Epoch 41 Batch 200 Loss 0.0868\n",
      "Epoch 41 Batch 300 Loss 0.0822\n",
      "Epoch 41 Batch 400 Loss 0.0941\n",
      "Epoch 41 Batch 500 Loss 0.1164\n",
      "Epoch 41 Batch 600 Loss 0.1104\n",
      "Epoch 41 Batch 700 Loss 0.1012\n",
      "Epoch 41 Batch 800 Loss 0.0840\n",
      "Epoch 41 Batch 900 Loss 0.0984\n",
      "Epoch 41 Batch 1000 Loss 0.0816\n",
      "Epoch 41 Batch 1100 Loss 0.0813\n",
      "Epoch 41 Batch 1200 Loss 0.0942\n",
      "Epoch 41 Batch 1300 Loss 0.1297\n",
      "Epoch 41 Batch 1400 Loss 0.0952\n",
      "Epoch 41 Batch 1500 Loss 0.0855\n",
      "Epoch 41 Batch 1600 Loss 0.0972\n",
      "Epoch 41 Batch 1700 Loss 0.1627\n",
      "Epoch 41 Batch 1800 Loss 0.1148\n",
      "Epoch 41 Batch 1900 Loss 0.1539\n",
      "Epoch 41 Batch 2000 Loss 0.1074\n",
      "Epoch 41 Batch 2100 Loss 0.1418\n",
      "Epoch 41 Batch 2200 Loss 0.1364\n",
      "Epoch 41 Batch 2300 Loss 0.1127\n",
      "Epoch 41 Batch 2400 Loss 0.1298\n",
      "Epoch 41 Batch 2500 Loss 0.1109\n",
      "Epoch 41 Batch 2600 Loss 0.1205\n",
      "Epoch 41 Batch 2700 Loss 0.1732\n",
      "Epoch 41 Batch 2800 Loss 0.1041\n",
      "Epoch 41 Batch 2900 Loss 0.1346\n",
      "Epoch 41 Batch 3000 Loss 0.1259\n",
      "Epoch 41 Batch 3100 Loss 0.1395\n",
      "Epoch 41 Batch 3200 Loss 0.0875\n",
      "Epoch 41 Batch 3300 Loss 0.1559\n",
      "Epoch 41 Batch 3400 Loss 0.1319\n",
      "Epoch 41 Batch 3500 Loss 0.0976\n",
      "Epoch 41 Batch 3600 Loss 0.0831\n",
      "Epoch 41 Batch 3700 Loss 0.1537\n",
      "Epoch 41 Loss 0.1111\n",
      "Time taken for 1 epoch 108.21154022216797 sec\n",
      "\n",
      "Epoch 42 Batch 0 Loss 0.0723\n",
      "Epoch 42 Batch 100 Loss 0.1029\n",
      "Epoch 42 Batch 200 Loss 0.0703\n",
      "Epoch 42 Batch 300 Loss 0.1539\n",
      "Epoch 42 Batch 400 Loss 0.0880\n",
      "Epoch 42 Batch 500 Loss 0.0810\n",
      "Epoch 42 Batch 600 Loss 0.1116\n",
      "Epoch 42 Batch 700 Loss 0.0712\n",
      "Epoch 42 Batch 800 Loss 0.0881\n",
      "Epoch 42 Batch 900 Loss 0.0855\n",
      "Epoch 42 Batch 1000 Loss 0.1021\n",
      "Epoch 42 Batch 1100 Loss 0.1166\n",
      "Epoch 42 Batch 1200 Loss 0.1088\n",
      "Epoch 42 Batch 1300 Loss 0.1344\n",
      "Epoch 42 Batch 1400 Loss 0.1133\n",
      "Epoch 42 Batch 1500 Loss 0.1110\n",
      "Epoch 42 Batch 1600 Loss 0.1004\n",
      "Epoch 42 Batch 1700 Loss 0.0810\n",
      "Epoch 42 Batch 1800 Loss 0.0989\n",
      "Epoch 42 Batch 1900 Loss 0.1269\n",
      "Epoch 42 Batch 2000 Loss 0.1528\n",
      "Epoch 42 Batch 2100 Loss 0.1219\n",
      "Epoch 42 Batch 2200 Loss 0.1171\n",
      "Epoch 42 Batch 2300 Loss 0.1050\n",
      "Epoch 42 Batch 2400 Loss 0.1498\n",
      "Epoch 42 Batch 2500 Loss 0.1057\n",
      "Epoch 42 Batch 2600 Loss 0.1062\n",
      "Epoch 42 Batch 2700 Loss 0.1324\n",
      "Epoch 42 Batch 2800 Loss 0.1200\n",
      "Epoch 42 Batch 2900 Loss 0.1297\n",
      "Epoch 42 Batch 3000 Loss 0.0800\n",
      "Epoch 42 Batch 3100 Loss 0.1099\n",
      "Epoch 42 Batch 3200 Loss 0.1193\n",
      "Epoch 42 Batch 3300 Loss 0.1144\n",
      "Epoch 42 Batch 3400 Loss 0.0951\n",
      "Epoch 42 Batch 3500 Loss 0.1017\n",
      "Epoch 42 Batch 3600 Loss 0.1004\n",
      "Epoch 42 Batch 3700 Loss 0.1044\n",
      "Epoch 42 Loss 0.1127\n",
      "Time taken for 1 epoch 110.27099561691284 sec\n",
      "\n",
      "Epoch 43 Batch 0 Loss 0.0849\n",
      "Epoch 43 Batch 100 Loss 0.0814\n",
      "Epoch 43 Batch 200 Loss 0.0485\n",
      "Epoch 43 Batch 300 Loss 0.0825\n",
      "Epoch 43 Batch 400 Loss 0.0793\n",
      "Epoch 43 Batch 500 Loss 0.0933\n",
      "Epoch 43 Batch 600 Loss 0.1041\n",
      "Epoch 43 Batch 700 Loss 0.0753\n",
      "Epoch 43 Batch 800 Loss 0.1098\n",
      "Epoch 43 Batch 900 Loss 0.1293\n",
      "Epoch 43 Batch 1000 Loss 0.1381\n",
      "Epoch 43 Batch 1100 Loss 0.0852\n",
      "Epoch 43 Batch 1200 Loss 0.0857\n",
      "Epoch 43 Batch 1300 Loss 0.0794\n",
      "Epoch 43 Batch 1400 Loss 0.0959\n",
      "Epoch 43 Batch 1500 Loss 0.1052\n",
      "Epoch 43 Batch 1600 Loss 0.1159\n",
      "Epoch 43 Batch 1700 Loss 0.1368\n",
      "Epoch 43 Batch 1800 Loss 0.1200\n",
      "Epoch 43 Batch 1900 Loss 0.1526\n",
      "Epoch 43 Batch 2000 Loss 0.0871\n",
      "Epoch 43 Batch 2100 Loss 0.1618\n",
      "Epoch 43 Batch 2200 Loss 0.1152\n",
      "Epoch 43 Batch 2300 Loss 0.1237\n",
      "Epoch 43 Batch 2400 Loss 0.0782\n",
      "Epoch 43 Batch 2500 Loss 0.1316\n",
      "Epoch 43 Batch 2600 Loss 0.1000\n",
      "Epoch 43 Batch 2700 Loss 0.0824\n",
      "Epoch 43 Batch 2800 Loss 0.1423\n",
      "Epoch 43 Batch 2900 Loss 0.1059\n",
      "Epoch 43 Batch 3000 Loss 0.0864\n",
      "Epoch 43 Batch 3100 Loss 0.1402\n",
      "Epoch 43 Batch 3200 Loss 0.1127\n",
      "Epoch 43 Batch 3300 Loss 0.1701\n",
      "Epoch 43 Batch 3400 Loss 0.1715\n",
      "Epoch 43 Batch 3500 Loss 0.1608\n",
      "Epoch 43 Batch 3600 Loss 0.1872\n",
      "Epoch 43 Batch 3700 Loss 0.1359\n",
      "Epoch 43 Loss 0.1136\n",
      "Time taken for 1 epoch 107.95123624801636 sec\n",
      "\n",
      "Epoch 44 Batch 0 Loss 0.1000\n",
      "Epoch 44 Batch 100 Loss 0.0936\n",
      "Epoch 44 Batch 200 Loss 0.1080\n",
      "Epoch 44 Batch 300 Loss 0.0945\n",
      "Epoch 44 Batch 400 Loss 0.0898\n",
      "Epoch 44 Batch 500 Loss 0.1090\n",
      "Epoch 44 Batch 600 Loss 0.0446\n",
      "Epoch 44 Batch 700 Loss 0.0705\n",
      "Epoch 44 Batch 800 Loss 0.1050\n",
      "Epoch 44 Batch 900 Loss 0.1011\n",
      "Epoch 44 Batch 1000 Loss 0.1446\n",
      "Epoch 44 Batch 1100 Loss 0.0839\n",
      "Epoch 44 Batch 1200 Loss 0.0777\n",
      "Epoch 44 Batch 1300 Loss 0.1318\n",
      "Epoch 44 Batch 1400 Loss 0.0823\n",
      "Epoch 44 Batch 1500 Loss 0.0857\n",
      "Epoch 44 Batch 1600 Loss 0.0988\n",
      "Epoch 44 Batch 1700 Loss 0.0938\n",
      "Epoch 44 Batch 1800 Loss 0.0976\n",
      "Epoch 44 Batch 1900 Loss 0.0951\n",
      "Epoch 44 Batch 2000 Loss 0.1213\n",
      "Epoch 44 Batch 2100 Loss 0.1012\n",
      "Epoch 44 Batch 2200 Loss 0.1163\n",
      "Epoch 44 Batch 2300 Loss 0.1477\n",
      "Epoch 44 Batch 2400 Loss 0.0807\n",
      "Epoch 44 Batch 2500 Loss 0.1126\n",
      "Epoch 44 Batch 2600 Loss 0.1662\n",
      "Epoch 44 Batch 2700 Loss 0.1075\n",
      "Epoch 44 Batch 2800 Loss 0.1084\n",
      "Epoch 44 Batch 2900 Loss 0.1131\n",
      "Epoch 44 Batch 3000 Loss 0.0931\n",
      "Epoch 44 Batch 3100 Loss 0.0932\n",
      "Epoch 44 Batch 3200 Loss 0.1184\n",
      "Epoch 44 Batch 3300 Loss 0.1487\n",
      "Epoch 44 Batch 3400 Loss 0.1671\n",
      "Epoch 44 Batch 3500 Loss 0.1063\n",
      "Epoch 44 Batch 3600 Loss 0.1210\n",
      "Epoch 44 Batch 3700 Loss 0.1254\n",
      "Epoch 44 Loss 0.1135\n",
      "Time taken for 1 epoch 110.46158814430237 sec\n",
      "\n",
      "Epoch 45 Batch 0 Loss 0.0808\n",
      "Epoch 45 Batch 100 Loss 0.0760\n",
      "Epoch 45 Batch 200 Loss 0.0740\n",
      "Epoch 45 Batch 300 Loss 0.0725\n",
      "Epoch 45 Batch 400 Loss 0.0896\n",
      "Epoch 45 Batch 500 Loss 0.0819\n",
      "Epoch 45 Batch 600 Loss 0.0970\n",
      "Epoch 45 Batch 700 Loss 0.1618\n",
      "Epoch 45 Batch 800 Loss 0.1245\n",
      "Epoch 45 Batch 900 Loss 0.1156\n",
      "Epoch 45 Batch 1000 Loss 0.0571\n",
      "Epoch 45 Batch 1100 Loss 0.1006\n",
      "Epoch 45 Batch 1200 Loss 0.0794\n",
      "Epoch 45 Batch 1300 Loss 0.1358\n",
      "Epoch 45 Batch 1400 Loss 0.1615\n",
      "Epoch 45 Batch 1500 Loss 0.1202\n",
      "Epoch 45 Batch 1600 Loss 0.1592\n",
      "Epoch 45 Batch 1700 Loss 0.1041\n",
      "Epoch 45 Batch 1800 Loss 0.1141\n",
      "Epoch 45 Batch 1900 Loss 0.1028\n",
      "Epoch 45 Batch 2000 Loss 0.1531\n",
      "Epoch 45 Batch 2100 Loss 0.1459\n",
      "Epoch 45 Batch 2200 Loss 0.0866\n",
      "Epoch 45 Batch 2300 Loss 0.1512\n",
      "Epoch 45 Batch 2400 Loss 0.0949\n",
      "Epoch 45 Batch 2500 Loss 0.1000\n",
      "Epoch 45 Batch 2600 Loss 0.1236\n",
      "Epoch 45 Batch 2700 Loss 0.0982\n",
      "Epoch 45 Batch 2800 Loss 0.1375\n",
      "Epoch 45 Batch 2900 Loss 0.1018\n",
      "Epoch 45 Batch 3000 Loss 0.1897\n",
      "Epoch 45 Batch 3100 Loss 0.1611\n",
      "Epoch 45 Batch 3200 Loss 0.1545\n",
      "Epoch 45 Batch 3300 Loss 0.1238\n",
      "Epoch 45 Batch 3400 Loss 0.1282\n",
      "Epoch 45 Batch 3500 Loss 0.1563\n",
      "Epoch 45 Batch 3600 Loss 0.1529\n",
      "Epoch 45 Batch 3700 Loss 0.1249\n",
      "Epoch 45 Loss 0.1139\n",
      "Time taken for 1 epoch 107.98277282714844 sec\n",
      "\n",
      "Epoch 46 Batch 0 Loss 0.0631\n",
      "Epoch 46 Batch 100 Loss 0.1140\n",
      "Epoch 46 Batch 200 Loss 0.0485\n",
      "Epoch 46 Batch 300 Loss 0.1184\n",
      "Epoch 46 Batch 400 Loss 0.1057\n",
      "Epoch 46 Batch 500 Loss 0.0740\n",
      "Epoch 46 Batch 600 Loss 0.0778\n",
      "Epoch 46 Batch 700 Loss 0.1047\n",
      "Epoch 46 Batch 800 Loss 0.0867\n",
      "Epoch 46 Batch 900 Loss 0.1129\n",
      "Epoch 46 Batch 1000 Loss 0.0771\n",
      "Epoch 46 Batch 1100 Loss 0.1004\n",
      "Epoch 46 Batch 1200 Loss 0.1177\n",
      "Epoch 46 Batch 1300 Loss 0.1751\n",
      "Epoch 46 Batch 1400 Loss 0.0936\n",
      "Epoch 46 Batch 1500 Loss 0.1351\n",
      "Epoch 46 Batch 1600 Loss 0.1193\n",
      "Epoch 46 Batch 1700 Loss 0.1038\n",
      "Epoch 46 Batch 1800 Loss 0.1094\n",
      "Epoch 46 Batch 1900 Loss 0.0832\n",
      "Epoch 46 Batch 2000 Loss 0.1365\n",
      "Epoch 46 Batch 2100 Loss 0.1365\n",
      "Epoch 46 Batch 2200 Loss 0.1070\n",
      "Epoch 46 Batch 2300 Loss 0.0923\n",
      "Epoch 46 Batch 2400 Loss 0.1295\n",
      "Epoch 46 Batch 2500 Loss 0.1453\n",
      "Epoch 46 Batch 2600 Loss 0.1316\n",
      "Epoch 46 Batch 2700 Loss 0.1089\n",
      "Epoch 46 Batch 2800 Loss 0.1732\n",
      "Epoch 46 Batch 2900 Loss 0.1411\n",
      "Epoch 46 Batch 3000 Loss 0.1498\n",
      "Epoch 46 Batch 3100 Loss 0.1413\n",
      "Epoch 46 Batch 3200 Loss 0.1190\n",
      "Epoch 46 Batch 3300 Loss 0.1492\n",
      "Epoch 46 Batch 3400 Loss 0.1913\n",
      "Epoch 46 Batch 3500 Loss 0.1483\n",
      "Epoch 46 Batch 3600 Loss 0.1185\n",
      "Epoch 46 Batch 3700 Loss 0.1239\n",
      "Epoch 46 Loss 0.1156\n",
      "Time taken for 1 epoch 110.41010975837708 sec\n",
      "\n",
      "Epoch 47 Batch 0 Loss 0.0933\n",
      "Epoch 47 Batch 100 Loss 0.1007\n",
      "Epoch 47 Batch 200 Loss 0.0803\n",
      "Epoch 47 Batch 300 Loss 0.1162\n",
      "Epoch 47 Batch 400 Loss 0.1219\n",
      "Epoch 47 Batch 500 Loss 0.0861\n",
      "Epoch 47 Batch 600 Loss 0.1390\n",
      "Epoch 47 Batch 700 Loss 0.1261\n",
      "Epoch 47 Batch 800 Loss 0.1221\n",
      "Epoch 47 Batch 900 Loss 0.0924\n",
      "Epoch 47 Batch 1000 Loss 0.0814\n",
      "Epoch 47 Batch 1100 Loss 0.1342\n",
      "Epoch 47 Batch 1200 Loss 0.1497\n",
      "Epoch 47 Batch 1300 Loss 0.0578\n",
      "Epoch 47 Batch 1400 Loss 0.1362\n",
      "Epoch 47 Batch 1500 Loss 0.1197\n",
      "Epoch 47 Batch 1600 Loss 0.1317\n",
      "Epoch 47 Batch 1700 Loss 0.0992\n",
      "Epoch 47 Batch 1800 Loss 0.1386\n",
      "Epoch 47 Batch 1900 Loss 0.1007\n",
      "Epoch 47 Batch 2000 Loss 0.1188\n",
      "Epoch 47 Batch 2100 Loss 0.1463\n",
      "Epoch 47 Batch 2200 Loss 0.1085\n",
      "Epoch 47 Batch 2300 Loss 0.1324\n",
      "Epoch 47 Batch 2400 Loss 0.1588\n",
      "Epoch 47 Batch 2500 Loss 0.1135\n",
      "Epoch 47 Batch 2600 Loss 0.1622\n",
      "Epoch 47 Batch 2700 Loss 0.1551\n",
      "Epoch 47 Batch 2800 Loss 0.1263\n",
      "Epoch 47 Batch 2900 Loss 0.1309\n",
      "Epoch 47 Batch 3000 Loss 0.1033\n",
      "Epoch 47 Batch 3100 Loss 0.1229\n",
      "Epoch 47 Batch 3200 Loss 0.0848\n",
      "Epoch 47 Batch 3300 Loss 0.1423\n",
      "Epoch 47 Batch 3400 Loss 0.1336\n",
      "Epoch 47 Batch 3500 Loss 0.1497\n",
      "Epoch 47 Batch 3600 Loss 0.0749\n",
      "Epoch 47 Batch 3700 Loss 0.1403\n",
      "Epoch 47 Loss 0.1148\n",
      "Time taken for 1 epoch 107.8989143371582 sec\n",
      "\n",
      "Epoch 48 Batch 0 Loss 0.0859\n",
      "Epoch 48 Batch 100 Loss 0.0714\n",
      "Epoch 48 Batch 200 Loss 0.0900\n",
      "Epoch 48 Batch 300 Loss 0.0742\n",
      "Epoch 48 Batch 400 Loss 0.0644\n",
      "Epoch 48 Batch 500 Loss 0.0830\n",
      "Epoch 48 Batch 600 Loss 0.0867\n",
      "Epoch 48 Batch 700 Loss 0.1333\n",
      "Epoch 48 Batch 800 Loss 0.0899\n",
      "Epoch 48 Batch 900 Loss 0.0891\n",
      "Epoch 48 Batch 1000 Loss 0.0972\n",
      "Epoch 48 Batch 1100 Loss 0.0909\n",
      "Epoch 48 Batch 1200 Loss 0.1088\n",
      "Epoch 48 Batch 1300 Loss 0.1382\n",
      "Epoch 48 Batch 1400 Loss 0.1351\n",
      "Epoch 48 Batch 1500 Loss 0.1678\n",
      "Epoch 48 Batch 1600 Loss 0.0970\n",
      "Epoch 48 Batch 1700 Loss 0.1108\n",
      "Epoch 48 Batch 1800 Loss 0.1257\n",
      "Epoch 48 Batch 1900 Loss 0.1080\n",
      "Epoch 48 Batch 2000 Loss 0.1195\n",
      "Epoch 48 Batch 2100 Loss 0.1224\n",
      "Epoch 48 Batch 2200 Loss 0.1010\n",
      "Epoch 48 Batch 2300 Loss 0.1273\n",
      "Epoch 48 Batch 2400 Loss 0.1052\n",
      "Epoch 48 Batch 2500 Loss 0.1613\n",
      "Epoch 48 Batch 2600 Loss 0.1581\n",
      "Epoch 48 Batch 2700 Loss 0.0907\n",
      "Epoch 48 Batch 2800 Loss 0.0764\n",
      "Epoch 48 Batch 2900 Loss 0.1766\n",
      "Epoch 48 Batch 3000 Loss 0.0886\n",
      "Epoch 48 Batch 3100 Loss 0.1738\n",
      "Epoch 48 Batch 3200 Loss 0.1583\n",
      "Epoch 48 Batch 3300 Loss 0.1538\n",
      "Epoch 48 Batch 3400 Loss 0.1683\n",
      "Epoch 48 Batch 3500 Loss 0.0986\n",
      "Epoch 48 Batch 3600 Loss 0.1601\n",
      "Epoch 48 Batch 3700 Loss 0.1389\n",
      "Epoch 48 Loss 0.1164\n",
      "Time taken for 1 epoch 110.38641786575317 sec\n",
      "\n",
      "Epoch 49 Batch 0 Loss 0.1342\n",
      "Epoch 49 Batch 100 Loss 0.0941\n",
      "Epoch 49 Batch 200 Loss 0.1109\n",
      "Epoch 49 Batch 300 Loss 0.1165\n",
      "Epoch 49 Batch 400 Loss 0.0796\n",
      "Epoch 49 Batch 500 Loss 0.0836\n",
      "Epoch 49 Batch 600 Loss 0.1026\n",
      "Epoch 49 Batch 700 Loss 0.0983\n",
      "Epoch 49 Batch 800 Loss 0.1050\n",
      "Epoch 49 Batch 900 Loss 0.1029\n",
      "Epoch 49 Batch 1000 Loss 0.0767\n",
      "Epoch 49 Batch 1100 Loss 0.1071\n",
      "Epoch 49 Batch 1200 Loss 0.1469\n",
      "Epoch 49 Batch 1300 Loss 0.0942\n",
      "Epoch 49 Batch 1400 Loss 0.1500\n",
      "Epoch 49 Batch 1500 Loss 0.0912\n",
      "Epoch 49 Batch 1600 Loss 0.1420\n",
      "Epoch 49 Batch 1700 Loss 0.1304\n",
      "Epoch 49 Batch 1800 Loss 0.1550\n",
      "Epoch 49 Batch 1900 Loss 0.1293\n",
      "Epoch 49 Batch 2000 Loss 0.1482\n",
      "Epoch 49 Batch 2100 Loss 0.1269\n",
      "Epoch 49 Batch 2200 Loss 0.0629\n",
      "Epoch 49 Batch 2300 Loss 0.1396\n",
      "Epoch 49 Batch 2400 Loss 0.1794\n",
      "Epoch 49 Batch 2500 Loss 0.0854\n",
      "Epoch 49 Batch 2600 Loss 0.1126\n",
      "Epoch 49 Batch 2700 Loss 0.1154\n",
      "Epoch 49 Batch 2800 Loss 0.1101\n",
      "Epoch 49 Batch 2900 Loss 0.1488\n",
      "Epoch 49 Batch 3000 Loss 0.1448\n",
      "Epoch 49 Batch 3100 Loss 0.0959\n",
      "Epoch 49 Batch 3200 Loss 0.1187\n",
      "Epoch 49 Batch 3300 Loss 0.1288\n",
      "Epoch 49 Batch 3400 Loss 0.1142\n",
      "Epoch 49 Batch 3500 Loss 0.1085\n",
      "Epoch 49 Batch 3600 Loss 0.1479\n",
      "Epoch 49 Batch 3700 Loss 0.1109\n",
      "Epoch 49 Loss 0.1184\n",
      "Time taken for 1 epoch 107.8747947216034 sec\n",
      "\n",
      "Epoch 50 Batch 0 Loss 0.0789\n",
      "Epoch 50 Batch 100 Loss 0.0836\n",
      "Epoch 50 Batch 200 Loss 0.1077\n",
      "Epoch 50 Batch 300 Loss 0.1076\n",
      "Epoch 50 Batch 400 Loss 0.0768\n",
      "Epoch 50 Batch 500 Loss 0.0878\n",
      "Epoch 50 Batch 600 Loss 0.0985\n",
      "Epoch 50 Batch 700 Loss 0.1287\n",
      "Epoch 50 Batch 800 Loss 0.1266\n",
      "Epoch 50 Batch 900 Loss 0.0826\n",
      "Epoch 50 Batch 1000 Loss 0.1346\n",
      "Epoch 50 Batch 1100 Loss 0.1289\n",
      "Epoch 50 Batch 1200 Loss 0.1294\n",
      "Epoch 50 Batch 1300 Loss 0.0708\n",
      "Epoch 50 Batch 1400 Loss 0.1109\n",
      "Epoch 50 Batch 1500 Loss 0.0984\n",
      "Epoch 50 Batch 1600 Loss 0.1441\n",
      "Epoch 50 Batch 1700 Loss 0.1049\n",
      "Epoch 50 Batch 1800 Loss 0.0963\n",
      "Epoch 50 Batch 1900 Loss 0.1892\n",
      "Epoch 50 Batch 2000 Loss 0.0861\n",
      "Epoch 50 Batch 2100 Loss 0.1106\n",
      "Epoch 50 Batch 2200 Loss 0.1989\n",
      "Epoch 50 Batch 2300 Loss 0.1427\n",
      "Epoch 50 Batch 2400 Loss 0.0922\n",
      "Epoch 50 Batch 2500 Loss 0.1767\n",
      "Epoch 50 Batch 2600 Loss 0.1052\n",
      "Epoch 50 Batch 2700 Loss 0.1247\n",
      "Epoch 50 Batch 2800 Loss 0.1340\n",
      "Epoch 50 Batch 2900 Loss 0.1294\n",
      "Epoch 50 Batch 3000 Loss 0.1085\n",
      "Epoch 50 Batch 3100 Loss 0.1286\n",
      "Epoch 50 Batch 3200 Loss 0.1166\n",
      "Epoch 50 Batch 3300 Loss 0.1739\n",
      "Epoch 50 Batch 3400 Loss 0.1525\n",
      "Epoch 50 Batch 3500 Loss 0.1822\n",
      "Epoch 50 Batch 3600 Loss 0.1305\n",
      "Epoch 50 Batch 3700 Loss 0.1768\n",
      "Epoch 50 Loss 0.1183\n",
      "Time taken for 1 epoch 110.28833627700806 sec\n",
      "\n",
      "CPU times: user 1h 54min 37s, sys: 23min 36s, total: 2h 18min 13s\n",
      "Wall time: 1h 30min 59s\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "\n",
    "%%time\n",
    "\n",
    "EPOCHS = 50\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)): \n",
    "        # inp это текст на русском а targ это текст на англ - то что мы ходим получить\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "cellId": "g4don1pfw8r5vmzhtxukss",
    "id": "EbQpyYs13jF_"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def evaluate(sentence):\n",
    "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "  result = ''\n",
    "\n",
    "  hidden = [tf.zeros((1, units))]\n",
    "  enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "  dec_hidden = enc_hidden\n",
    "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "  for t in range(max_length_targ):\n",
    "    predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
    "\n",
    "    # storing the attention weights to plot later on\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "    result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "    if targ_lang.index_word[predicted_id] == '<end>':\n",
    "      return result, sentence\n",
    "\n",
    "    # the predicted ID is fed back into the model\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "  return result, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "cellId": "nlgxaienp2groe8putguo",
    "id": "sl9zUHzg3jGI"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def translate(sentence):\n",
    "  result, sentence = evaluate(sentence)\n",
    "\n",
    "  print('Input: %s' % (sentence))\n",
    "  print('Predicted translation: {}'.format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "cbmhhzuydeuejng0yiebjf",
    "execution_id": "88a3ced3-a2d7-4a92-8343-526f08fdf9ac",
    "id": "n250XbnjOaqP"
   },
   "source": [
    "## Restore the latest checkpoint and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "cellId": "6ou4v2bsuzfd7kcypibtvd",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UJpT9D5_OgP6",
    "outputId": "fa01e136-763c-4b3f-ddef-fc2322ffbebf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f802cfc6610>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "86rzqoozqlggp76iql4qj",
    "execution_id": "b1fb9368-0d22-4a06-a0b9-9cdae7495501",
    "id": "km5ACNv7X5V3"
   },
   "source": [
    "## Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "cellId": "opyyo6qap4dmzrf99yz55",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "id": "Lof86vWhX--X",
    "outputId": "30b3d963-cb10-4a8a-9832-ba7c6613abdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> здесь все классно . <end>\n",
      "Predicted translation: it's all here funny . <end> \n",
      "Input: <start> я не успеваю делать домашку . <end>\n",
      "Predicted translation: i don't have time for my homework . <end> \n",
      "Input: <start> когда я буду много получать денег ? <end>\n",
      "Predicted translation: when did i buy a lot of money ? <end> \n",
      "Input: <start> я учусь учить модели . <end>\n",
      "Predicted translation: i'm making model history . <end> \n",
      "Input: <start> береги природу , мать твою . <end>\n",
      "Predicted translation: please love your mother . <end> \n",
      "Input: <start> перевод можно сделать и лучше <end>\n",
      "Predicted translation: the place can only be happy . <end> \n",
      "Input: <start> выберите новый ноутбук по вашим возможностям . <end>\n",
      "Predicted translation: choose your new haircut . <end> \n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "# Тестируем\n",
    "\n",
    "test_texts = ['Здесь все классно.', 'Я не успеваю делать домашку.', 'Когда я буду много получать денег?',\n",
    "              'Я учусь учить модели.', 'Береги природу, мать твою.',\n",
    "              'Перевод можно сделать и лучше', 'Выберите новый ноутбук по Вашим возможностям.',\n",
    "              ]\n",
    "\n",
    "for text in test_texts:\n",
    "    translate(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "6sw0gj2sv6k51wsq49o3gf",
    "execution_id": "05c87922-45b6-4946-ad89-097b81b265fa",
    "id": "jdXES85KkTVS"
   },
   "source": [
    "Результаты ну такое себе, но даже немного забавно\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "f9e35a1d-e5fd-47db-888a-2694657e2301",
  "notebookPath": "PopovMV_NLP_HW_10.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
